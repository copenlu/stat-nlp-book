{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Exercises\n",
    "In these exercises you will extend and develop language models. We will use the code from the notes, but within a python package [`lm`](http://localhost:8888/edit/statnlpbook/lm.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T16:57:44.612226",
     "start_time": "2016-10-21T16:57:44.592461"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "_snlp_book_dir = \"..\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "from statnlpbook.lm import *\n",
    "from statnlpbook.ohhla import *\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T16:57:44.672863",
     "start_time": "2016-10-21T16:57:44.627958"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "docs = load_all_songs(\"../data/ohhla/train/www.ohhla.com/anonymous/j_live/\")\n",
    "assert len(docs) == 50, \"Your ohhla corpus is corrupted, please download it again!\"\n",
    "trainDocs, testDocs = docs[:len(docs)//2], docs[len(docs)//2:] \n",
    "train = words(trainDocs)\n",
    "test = words(testDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Optimal Pseudo Count \n",
    "\n",
    "Plot the perplexity for laplace smoothing on the given data as a function of alpha in the interval [0.001, 0.1] in steps by 0.001. Is it fair to assume that this is a convex function? Write a method that finds the optimal pseudo count `alpha` number for [laplace smoothing](https://github.com/uclmr/stat-nlp-book/blob/python/statnlpbook/lm.py#L180) for the given data up to some predefined numerical precision `epsilon` under the assumption that the perplexity is a convex function of alpha. How often did you have to call `perplexity` to find the optimum?\n",
    "\n",
    "Tips:\n",
    "<font color='white'>\n",
    "You don't need 1st or 2nd order derivatives in this case, only the gradient descent direction. Think about recursively slicing up the problem.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T16:57:55.326750",
     "start_time": "2016-10-21T16:57:44.674646"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "0.5 0.0\n",
      "0.25 0.0\n",
      "0.125 0.0\n",
      "0.0625 0.0\n",
      "0.03125 0.0\n",
      "0.03125 0.015625\n",
      "0.0234375 0.015625\n",
      "0.0234375 0.01953125\n",
      "0.0234375 0.021484375\n",
      "0.0234375 0.0224609375\n",
      "0.02294921875 0.0224609375\n",
      "0.022705078125 0.0224609375\n",
      "0.0225830078125 0.0224609375\n",
      "0.02252197265625 0.0224609375\n",
      "0.022491455078125 0.0224609375\n",
      "0.022491455078125 0.0224761962890625\n",
      "0.02248382568359375 0.0224761962890625\n",
      "0.02248382568359375 0.022480010986328125\n",
      "0.02248382568359375 0.022481918334960938\n",
      "0.022482872009277344 0.022481918334960938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.022482872009277344, 65.3568849083981)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAFwCAYAAABkcQUaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X2U3dVd7/H3N4QQEiAQSCAQCAkQEh6SQBJorch44bIq\nesWLq1iqrWiLLJUW26W2VKlZanXRWqr22muxLaUP0lbs42ql3EoHXFVpngkhD1DKQwgJeU7IM8m+\nf+xzOjPJhJnM+c35nd+Z92utvc6ZM+d3ft9Zv5WZT/bev70jpYQkSZKKM6zsAiRJktqNAUuSJKlg\nBixJkqSCGbAkSZIKZsCSJEkqmAFLkiSpYP0KWBFxR0Qsq7X31F77s4hYExGLau3Ng1uqJElSNQzv\n6w0RcTHwTmAO8BrwbxHxndq370kp3TOI9UmSJFVOnwELmA48nlLaCxARjwE31r4Xg1WYJElSVfVn\niPBJ4KqIOCUiRgHXAxOBBNweEUsi4tMRMWYwC5UkSaqK6M9WORHxW8DvA68Cy4G9wF8DG1NKKSL+\nEpiQUnrnYBYrSZJUBf0KWD0OiPgw8GJK6R+7vTYJ+HZKaUYv73ezQ0mSVBkppYanQPVnDhYRMS6l\ntCEizgH+N/CGiDgjpbSu9pYbyUOJRyq00TpVknnz5jFv3ryyy9AAeO2qzetXXV67aosoZnp5vwIW\n8K8RMRbYD/xeSml7RPyfiJgFHASeA24rpCJJkqSK61fASin9XC+vvaP4ciRJkqrPldz1ujo6Osou\nQQPktas2r191ee0EA5jkftQniEjOwZIkSVUQEYVMcrcHS5IkqWAGLEmSpIIZsCRJkgpmwJIkSSqY\nAUuSJKlgBixJkqSClRKwtm8HV26QJEntqpSANW4c7N1bxpklSZIGXykBa/Ro2LmzjDNLkiQNPgOW\nJElSwQxYkiRJBTNgSZIkFcyAJUmSVDADliRJUsEMWJIkSQUzYEmSJBXMgCVJklSw0gLWq6+WcWZJ\nkqTBZw+WJElSwQxYkiRJBTNgSZIkFcyAJUmSVDADliRJUsEMWJIkSQUzYEmSJBXMgCVJklQwA5Yk\nSVLBDFiSJEkFM2BJkiQVzIAlSZJUsFIC1ogRkBLs21fG2SVJkgZXKQErwl4sSZLUvkoJWGDAkiRJ\n7cuAJUmSVDADliRJUsEMWJIkSQUzYEmSJBXMgCVJklQwA5YkSVLBDFiSJEkFM2BJkiQVzIAlSZJU\nMAOWJElSwQxYkiRJBTNgSZIkFaxfASsi7oiIZbX2ntprp0TEwxGxKiK+FxFjjubEBixJktSu+gxY\nEXEx8E5gDjAL+KWIOA/4APD9lNKFwCPAnUdz4hNOMGBJkqT21J8erOnA4ymlvSmlA8BjwI3ALwP3\n195zP/ArR3Nie7AkSVK76k/AehK4qjYkOAq4HjgbOD2ltB4gpbQOGH80JzZgSZKkdjW8rzeklFZG\nxN3A/wNeBRYDB3p769Gc2IAlSZLaVZ8BCyCldB9wH0BEfBh4EVgfEaenlNZHxBnAK0c6ft68eT99\n3tHRQUdHhwFLkiSVrrOzk87OzsI/N1Lqu+MpIsallDZExDnAQ8AbgD8BNqeU7o6I9wOnpJQ+0Mux\nqbdzbNwIF14ImzY1/DNIkiQVIiJIKUXDn9PPgPUYMBbYD7w3pdQZEWOBr5LnYz0P3JRS2trLsb0G\nrN274ZRTYM+eBn8CSZKkgjQ1YDV0giMErJRg+HDYtw+OOWZQS5AkSeqXogJWaSu5R8CoUc7DkiRJ\n7ae0gAXeSShJktqTAUuSJKlgBixJkqSCGbAkSZIKZsCSJEkqmAFLkiSpYAYsSZKkghmwJEmSCmbA\nkiRJKpgBS5IkqWAGLEmSpIIZsCRJkgpmwJIkSSqYAUuSJKlgBixJkqSClR6wXn21zAokSZKKV3rA\nsgdLkiS1GwOWJElSwQxYkiRJBTNgSZIkFcyAJUmSVLBSA9aoUbBnDxw8WGYVkiRJxSo1YA0bBiNH\nwq5dZVYhSZJUrFIDFjhMKEmS2o8BS5IkqWAGLEmSpIIZsCRJkgpmwJIkSSqYAUuSJKlgBixJkqSC\nGbAkSZIKZsCSJEkqmAFLkiSpYAYsSZKkghmwJEmSCmbAkiRJKpgBS5IkqWAGLEmSpIIZsCRJkgpm\nwJIkSSqYAUuSJKlgBixJkqSCGbAkSZIKZsCSJEkqWEsErF27IKWyK5EkSSpG6QFr+PDc9uwpuxJJ\nkqRi9CtgRcSdEbE8Ip6IiC9FxHER8WcRsSYiFtXamwdahMOEkiSpnfQZsCJiEnArcFlKaQYwHHhr\n7dv3pJQur7WHBlqEAUuSJLWT/vRgbQf2AaMjYjgwCnip9r0ooggDliRJaid9BqyU0hbgY8AL5GC1\nNaX0/dq3b4+IJRHx6YgYM9AiDFiSJKmd9GeIcArwXmAScCZwQkS8DfgkMCWlNAtYB9wz0CIMWJIk\nqZ0M78d75gA/TCltBoiIrwE/k1L6527v+Sfg20f6gHnz5v30eUdHBx0dHT2+b8CSJEll6OzspLOz\ns/DPjdTHAlQRMRP4IjAX2AvcB8wH/jWltK72nvcCc1NKb+vl+NTXOd7yltxuumlAP4MkSVIhIoKU\nUsNzzPvswUopLY2IzwMLgQPAIuBe4DMRMQs4CDwH3DbQIuzBkiRJ7aQ/Q4SklD4KfPSQl99RVBEG\nLEmS1E5KX8kdDFiSJKm9GLAkSZIKZsCSJEkqmAFLkiSpYAYsSZKkghmwJEmSCmbAkiRJKpgBS5Ik\nqWAGLEmSpIIZsCRJkgpmwJIkSSqYAUuSJKlgLRWwUiq7EkmSpMa1RMAaMSI/7ttXbh2SJElFaImA\nBQ4TSpKk9mHAkiRJKpgBS5IkqWAGLEmSpIIZsCRJkgpmwJIkSSqYAUuSJKlgBixJkqSCGbAkSZIK\nZsCSJEkqmAFLkiSpYAYsSZKkghmwJEmSCtYyAevkk2HLlrKrkCRJalzLBKwzzoB168quQpIkqXEt\nE7AmTDBgSZKk9tAyAcseLEmS1C5aJmCNHQu7dsGePWVXIkmS1JiWCVgRcPrp9mJJkqTqa5mABQ4T\nSpKk9mDAkiRJKlhLBawJE+Dll8uuQpIkqTEtFbDswZIkSe3AgCVJklSwlgpYDhFKkqR20FIByx4s\nSZLUDgxYkiRJBYuU0uCeICL19xx79sBJJ8HevXnhUUmSpGaKCFJKDaeQlurBGjkSRo+GzZvLrkSS\nJGngWipggcOEkiSp+louYHknoSRJqrqWC1j2YEmSpKozYEmSJBWsXwErIu6MiOUR8UREfCkiRkTE\nKRHxcESsiojvRcSYIgqaMMGAJUmSqq3PgBURk4BbgctSSjOA4cDNwAeA76eULgQeAe4soqAzznAO\nliRJqrb+9GBtB/YBoyNiOHA88BJwA3B/7T33A79SREEOEUqSpKrrM2CllLYAHwNeIAerbSml7wOn\np5TW196zDhhfREEOEUqSpKob3tcbImIK8F5gErAN+JeI+HXg0OXZj7hc+7x58376vKOjg46OjiOe\nzyFCSZLULJ2dnXR2dhb+uX1ulRMRNwH/M6V0a+3rtwNvAP4H0JFSWh8RZwA/SClN7+X4fm+VA3Dw\nYF7RfccOOO64o/hJJEmSGtTMrXJWAW+IiJEREcA1wFPAt4Bbau/5TeCbjRYDMGwYnH46rF9fxKdJ\nkiQ1X59DhCmlpRHxeWAhcABYDNwLnAh8NSJ+G3geuKmoourDhOecU9QnSpIkNU+fAQsgpfRR4KOH\nvLwZuLbwivBOQkmSVG0tt5I7eCehJEmqtpYMWN5JKEmSqqxlA5Y9WJIkqapaMmA5RChJkqqsJQOW\nQ4SSJKnKWjZg2YMlSZKqqs+V3Bs+wVGu5A6wezecfDLs2QPR8FqqkiRJ/dPMldyb7vjjc9u6texK\nJEmSjl5LBixwHpYkSaqulg5YzsOSJElV1LIBy6UaJElSVbVswHKIUJIkVVVLByx7sCRJUhW1bMBy\niFCSJFVVywYshwglSVJVtXTAsgdLkiRVUcsGLIcIJUlSVbXkVjkABw/CccfBzp0wYsQgFCZJknSI\ntt4qB2DYMBg/HtavL7sSSZKko9OyAQscJpQkSdXU0gHLOwklSVIVtXzAsgdLkiRVTUsHLIcIJUlS\nFbV0wLIHS5IkVVHLByznYEmSpKpp6YDlEKEkSaqilg5YDhFKkqQqatmV3AF27YJTT82P0fCaqpIk\nSa+v7VdyBxg1Km+Ts21b2ZVIkiT1X0sHLHCiuyRJqp6WD1hTpsAzz5RdhSRJUv+1fMCaPh1WrCi7\nCkmSpP4zYEmSJBWs5QPWRRfBU0+VXYUkSVL/tfQyDQCbN8PkybB1q0s1SJKkwTUklmkAGDsWRo6E\ntWvLrkSSJKl/Wj5gQR4mdB6WJEmqikoErOnTnYclSZKqozIByx4sSZJUFQYsSZKkglUiYLlUgyRJ\nqpJKBKwJE2DvXti0qexKJEmS+laJgBXhMKEkSaqOSgQscJhQkiRVR2UClj1YkiSpKgxYkiRJBRve\n1xsiYirwFSABAUwB7gJOAW4FXqm99YMppYcGqU5Xc5ckSZVxVJs9R8QwYA1wJfDbwI6U0j19HNPQ\nZs91Bw7AiSfCK6/ACSc0/HGSJEmHKWuz52uBH6eUXqzX0WgB/XXMMTB1Kqxc2awzSpIkDczRBqxf\nAx7o9vXtEbEkIj4dEWMKrKtXzsOSJElV0O+AFRHHAr8M/EvtpU8CU1JKs4B1wOsOFRbBpRokSVIV\n9DnJvZtfABamlDYA1B9r/gn49pEOnDdv3k+fd3R00NHRcVRF1k2fDl/84oAOlSRJOkxnZyednZ2F\nf26/J7lHxAPAQyml+2tfn5FSWld7/l5gbkrpbb0cV8gkd4Dly+HGG2HVqkI+TpIkqYeiJrn3K2BF\nxCjgefKQ4I7aa58HZgEHgeeA21JK63s5trCAtW8fjBkDW7fCcccV8pGSJEk/1dSA1dAJCgxYANOm\nwYMPwiWXFPaRkiRJQHnLNJTOOwklSVKrM2BJkiQVrHIBy6UaJElSq6tcwLIHS5IktbrKTXLfuRPG\njYMdO/L2OZIkSUUZspPcR4+G8ePhuefKrkSSJKl3lQtYkIcJnYclSZJaVWUDlvOwJElSqzJgSZIk\nFaySAeuSS+CJJ8quQpIkqXeVu4sQYO9eOPVUWLsWTjqp0I+WJElD2JC9ixDyRs9z5sB//mfZlUiS\nJB2ukgEL4Od+Dh57rOwqJEmSDlfZgHX11QYsSZLUmio5Bwvyiu6nnw6vvAKjRhX+8ZIkaQga0nOw\nIK/ofuml8PjjZVciSZLUU2UDFjgPS5IktSYDliRJUsEqOwcLYNs2mDgRNm2CESMG5RSSJGkIGfJz\nsADGjIELLoAFC8quRJIkqUulAxY4TChJklqPAUuSJKlglZ6DBbBhA5x/fp6HNXz4oJ1GkiQNAc7B\nqhk3Lk90X7q07EokSZKyygcscJhQkiS1FgOWJEkaMlKC55+H//iPwT1P5edgAaxZAzNn5vlYw9oi\nMkqSpEallDPCwoV5SacFC/Lz4cPh2mvhC184/Jii5mC1RcACOO88+OY34ZJLBv1UkiSpxaQEa9ce\nHqYA5syB2bPz45w5cOaZR/6cogJW29x3d/XVeZjQgCVJUvtbt64rRNUD1WuvdYWp3/md/PyssyAa\njktHr216sD73OXjoIfjylwf9VJIkqYk2bOgZpBYsgF27unqk6u3ssxsPUw4RHuLZZ+FnfxZeeqmc\npCpJkhq3ZcvhYWrr1p5DfLNnw+TJg/P33oB1iJRycv3BD/L+hJIkqbVt3w6LFvUMU+vXw+WX9wxT\n55/fvJvYDFi9ePvb4Y1vhN/7vaacTpIk9dOrr8KSJT3DVH0VgHqQmjsXpk6FY44pr04DVi++8Q34\n27+Fzs6mnE6SJPViz568w8r8+V1h6ic/yTei1YPUnDkwfXrrbXNnwOrFnj0wYQI8+WS+a0CSJA2u\nffvy390FC7oC1apVMG1azzB1ySUwYkTZ1fbNgHUEt9wCs2bBH/xB004pSdKQ8NprsGJFV6/U/Pmw\nfHmecD53bleYmjEDRo4su9qBMWAdwb/9G/z5n8N//VfTTilJUts5eBCefrpnz9SSJXmEqB6k5s7N\nnRqjR5ddbXEMWEewf38eJlywAM49t2mnlSSpslKC557rGaYWLoSxY7uCVH0i+pgxZVc7uAxYr+O2\n2/LWOX/8x009rSRJlbB2bQ5S3SehH3fc4WFq3LiyK20+A9breOQR+MM/zGtrSJI0lG3c2HPO1Pz5\neWJ69zlTfe3PN5QYsF7HgQMwcSI8+mheT0OSpKFg27bcuVDvmZo/HzZv7nk339y5MGmSu54ciQGr\nD+95T+7avOuupp9akqRBt3s3LF7cs2eq+8Kd9R6qCy5o3iro7cCA1Ycf/jDvpL18edNPLUlSofbv\nh2XLes6bWr06L9TZfajv4otbb+HOqjFg9eHgwXwX4Xe/mxc3kySpCg4cyAt11sPU/Pl5Ic/6WlP1\n3qkqrzXVygxY/fBHf5TvivjLvyzl9JIkva6U8hYy3Yf5Fi3KU1zqPVNz5+bNj084oexqhwYDVj8s\nWAA335y7UZ3MJ0kq28sv9+yZqi+P0D1MzZmT159SOQxY/ZBSntz3la/kOygkSWqWLVt69kzNn58n\npncPUnPnujxCq2lawIqIqcBXgAQEMAW4C/hC7fVJwHPATSmlbb0cX1rAAviTP8mTAz/ykdJKkCS1\nuZ078x193cPUunV5aK9779TkyY6otLpSerAiYhiwBrgSuB3YlFL6SES8HzglpfSBXo4pNWAtWwbX\nX5/HuL2zQpLUqH37et7RN38+PPNMvoOvHqSuuAKmTYNjjim7Wh2tsgLWdcBdKaWrImIlcHVKaX1E\nnAF0ppSm9XJMqQEL4Kqr4Pbb4dd+rdQyJEkVc/AgrFx55Dv66m3GjDyXStVXVsD6DLAgpfR/I2JL\nSumUbt/bnFI6bFpeKwSsb38b5s3LY+F2zUqSepMSPP98zzC1aBGcdpp39A0lTQ9YEXEssBaYnlLa\neGigiohNKaVTezmu9IB18GBeC+sTn4Brrim1FElSi3jllZ5hav78vOL5FVd0DfPNmQOnHvaXTe2s\nqIB1NLOSfgFYmFLaWPt6fUSc3m2I8JUjHThv3ryfPu/o6KCjo2MApQ7csGF5TayPfMSAJUlD0Y4d\nsHAh/OhHXWFq27auO/ne9S741KfgrLMc6RhqOjs76ezsLPxzj6YH6wHgoZTS/bWv7wY2p5TubuVJ\n7nV798KUKfCd78CsWWVXI0kaLHv3wtKlPcPUCy/kPfq6D/Wdf7579OlwTR0ijIhRwPPAlJTSjtpr\nY4GvAmfXvndTSmlrL8e2RMAC+OhHYckS+NKXyq5EklSEAwdgxYocouqBasUKmDq1Z5i6+GI49tiy\nq1UVuNDoAGzblnuxFi7M+xRKkqqjvq1M9zC1eHFeqLN7mJo1C0aNKrtaVZUBa4De/37Yswf+7u/K\nrkSS9HrWresa4vvRj/Kd4CNHdk1Cr6+GfvLJZVeqdmLAGqC1a/MdhU8/7Z0hktQqtm3r2lam3ju1\nc2cOUN0DldvKaLAZsBrwznfmIcK77iq7EkkaevbsyfNhu4epNWvy0F73MHXeed7Rp+YzYDVgxQro\n6IDnnoPjjy+7GklqX6+9Bk891XOtqRUr8jYy3cPURRe5nZlagwGrQTfcANdeC+9+d9mVSFJ7SAl+\n/OOeYWrxYpg4secefTNn+p9btS4DVoOWLoXrroPly/M2CJKko7N2bc8wtWBBvnuvHqTmzoXZs52E\nrmoxYBXgjjtg9264996yK5Gk1rZlS9ck9Hrbvbvn8ghXXAFnnFF2pVJjDFgF2LYNpk+Hr38drryy\n7GokqTXs2pUnoXdfCf3ll+Gyy3r2Tk2e7CR0tR8DVkG++EX4+MfzL5Jjjim7Gklqrv374ckne/ZM\nrV6dJ51375maPt3fkRoaDFgFSQmuvhpuvhl+93fLrkaSBs/Bg3kNwO5haunSvGxN96G+GTPygp7S\nUGTAKtCyZXDNNfl/cePHl12NJDUuJXjxxZ5hauHCPOG8+/IIs2fDiSeWXa3UOgxYBXvf+2DrVvjs\nZ8uuRJKO3oYNPcPU/Pn59e49U3Pnwrhx5dYptToDVsG2b89zDL76VXjTm8quRpKObPv23BvVPUxt\n3Zp7o7qHqbPPdhK6dLQMWIPggQfg7rvzL6tjjy27GknKSyEsWdJziYQXXsiLdXYPUxdcAMOGlV2t\nVH0GrEGQEvzSL+WerL/5m7KrkTTU7N+fFz/u3jO1alX+nTRnTleYuvhit5WRBosBa5Bs2pS72e+5\nB268sexqJLWrAwdyeKr3TC1YAE88ke/o6x6mZs70jj6pmQxYg2j+fLj+evjhD2Hq1LKrkVR1KcFP\nfnL4Hn3jx/cMU5dd5h19UtkMWIPsH/8R/uEf4L//G0aPLrsaSVWREqxZk3ukuvdOjR6dQ1Q9UM2e\nDWPHll2tpEMZsAZZSvCbv5mf33+/d+JI6t26dV1hqt4OHuzqlZozJzf36JOqwYDVBLt25T0Kb78d\nbrut7GoklW3jxhygFi7sClM7d3aFqHqgmjjR/5RJVWXAapLVq/O6WN/9bv7lKWlo2LKlZ5BasCC/\nNnt2V6CaPRumTDFMSe3EgNVEX/967sV65BG48MKyq5FUtK1bYdGirkC1cCGsXw+XX55DVH0Bz/PP\nd60pqd0ZsJrsvvvgQx+Czk4477yyq5E0UNu3Hx6m1q6FWbO6eqXmzMl3EB9zTNnVSmo2A1YJPvUp\n+Ou/ziHr3HPLrkZSX7qHqXqgWrsWZszoOdQ3bZphSlJmwCrJJz4BH/84PPpo3udLUmvYti2vLXVo\nz1Q9TNUD1bRproIu6cgMWCX62Mdyb1ZnJ5x5ZtnVSEPP5s25Z6reO7VoEbz8cg5T9WG+2bMNU5KO\nngGrZH/1V/CFL8C//7shSxpMGzb0DFILF+blEmbN6gpSl1/uMJ+kYhiwWsDdd+chw699Da64ouxq\npGpLCV56qatnavHi/LhjR9fdfJdfntv55xumJA0OA1aL+OY34V3vyvOyfuM3yq5GqoaU4NlnDw9T\nKfUMUpdfDpMnu86UpOYxYLWQZcvghhvgLW/JQ4f+z1rqsn8/rFiRQ1S9LV0KJ52UA9Rll3U9nnWW\nYUpSuQxYLWbjxhywRo+Gf/7n/MdDGmp27IAnnoAlS3KQWrIEnnoKJk3KAareZs2C004ru1pJOpwB\nqwXt3w933JEnvn/uc/DGN5ZdkTQ4Usp37S1Z0rO99BJcfHEOUPUgNWNG/o+HJFWBAauFPfggvPvd\n8La3wV/8BYwaVXZF0sDt3w8rV+ZhvaVLc5BaujSHrFmzutrMmS6LIKn6DFgtbuPGHLIWLoTPfAau\nuqrsiqS+bdqUw9MTT3QFqpUr86K69RA1c2Z+fuaZzpeS1H4MWBXxjW/A7/8+/Oqv5gnwJ5xQdkVS\n7pVatSoHqe5tx448pFdvs2bBJZc4xCdp6DBgVcjmzfC+98H3vgd/+qdw660wYkTZVWkoSAnWrMl3\nui5bBk8+mYPU6tV54vmMGXDppflx5sz8mr1SkoYyA1YFLV4Md94JTz+d52a99a0wbFjZValdbN6c\nA1S91QPVccflEFVvM2bARRfB8ceXXbEktR4DVoX94Ac5aO3ZAx/+MFx/vb0G6r/t22H58sPbjh15\nOK97u/RSGDeu7IolqToMWBWXUl4F/kMfggMH8oT4t7/duS7qsmVLXqDzqae62vLl+fXp0/NyCBdd\nlB8vvTRPRDeoS1JjDFhtIiV49FH4+7+Hxx6DW27Jk+InTy67MjVDSrB2bQ5SK1bkO/bqz199NQeo\n7m36dDj3XIeWJWmwGLDa0HPPwSc/CZ/9bN48+uab8xY8rgpffbt357l3q1bltnJl1/Pjj8/Bafr0\nvI5U/dEeKUlqPgNWG9u1C771LXjgAejshGuvzRPif/EXXbS0le3bl0PyM8/ku/RWr86havVqWL8e\npkzJwenCC3OrPz/llLIrlyTVGbCGiC1b8lpaX/4yPP44XH01XHddbuefbw9Hs+3cCc8+m9uPf5zb\nM8/ktmYNTJyYr8vUqT3bOee4CbgkVYEBawjauDHvc/i978HDD+ctSa67Dq65Bq680jWMirBvH7z4\nIvzkJ7k3qvvjs8/Ctm15ftyUKXDeefnxggtyqDr3XNc3k6SqM2ANcSnlidAPP5yHER9/PL92xRU5\nbF15ZV6F+7TTyq60dRw4AK+8knua6u2FF3q2DRvgrLNyWJo8OT/Wn0+ZAhMmOMFcktqZAUs9pJR7\nXh5/vKstW5Z7VLrfhTZtWg4MZ5+dF6BsB3v25GC0YQOsWwcvv3x4W7s2P44dmwPUxIm5nXNObpMm\n5ccJExzKk6ShzIClPqWUA0f3dZRWrsxDXmvXwqmn5mAxaVIOHePGwfjxPR9POim3wR76OnAgz2/a\nti0vpLltW1fbujVvQrxpU16tvP58w4bcI7V7d1fNp5+eQ9Kh7ayz8ubEDuFJkl5PUwNWRIwBPg1c\nAhwEfht4M3Ar8ErtbR9MKT3Uy7EGrBZ04EAOWc8/n9vLL+ewUg8t9R6h7dtzGzYMTjwxh61Ro3Lv\n14gRPVtvQ2cHD+aNhffty63+fM+eHKh27cqPr72WP/ekk2DMmJ7t5JNzGBw7tutx7NgcqMaPz993\n7pkkqQjNDlifAx5NKd0XEcOB0cAfADtSSvf0cawBq8I6Ozu5+uoO9u7NW7Fs354DUffQtG8f7N2b\ne8wOFZHD17HHdgWxY4/Naz+NGpXb6NE5sBmSitXZ2UlHR0fZZWiAvH7V5bWrtqIC1vB+nOgk4KqU\n0i0AKaXXgG2R/xr6J7HN1X9RjBwJI0e6r12V+Eu+2rx+1eW1E0B/7oeaDGyMiPsiYlFE3BsR9eUu\nb4+IJRHx6dowoiRJ0pDXn4A1HLgc+IeU0uXALuADwCeBKSmlWcA64HWHCiVJkoaKPudgRcTpwH+l\nlKbUvv5Z4P0ppf/V7T2TgG+nlGb0crwTsCRJUmU0ZQ5WSml9RLwYEVNTSquBa4CnIuKMlNK62ttu\nBJ4crCIhmZD7AAADxUlEQVQlSZKqpL93Ec4kL9NwLPAs8FvAJ4BZ5GUbngNuSymtH7RKJUmSKmLQ\nFxqVJEkaaga8q1pEvDkiVkbE6oh4/xHe8/cR8XTtTsNZR3OsBtdAr19ETIyIRyJieUQsi4j3NLdy\nQWP//mrfG1a7K/hbzalYdQ3+7hwTEf8SEStq/wavbF7lgoav35216/ZERHwpItxboon6unYRcWFE\n/GdE7ImI9x3Nsb1KKR11IwezZ4BJ5GHDJcC0Q97zC8B3as+vBP67v8faBrc1eP3OAGbVnp8ArPL6\nVef6dfv+e4EvAt8q++cZSq3Rawd8Dvit2vPhwEll/0xDqTX4u3MSeYrNiNrXXwHeUfbPNFRaP6/d\nacBs4C+A9x3Nsb21gfZgXQE8nVJ6PqW0H/gycMMh77kB+DxASulxYEztjsT+HKvBNeDrl1Jal1Ja\nUnv9VWAFcFbzSheN/fsjIiYC15PnVaq5Bnztui36fF/te6+llLY3sXY19m9vO7APGF3bEWUUsLZp\nlavPa5dS2phSWgi8drTH9magAess4MVuX6/h8D+yR3pPf47V4BrI9Xvp0PdExLnkGx0eL7xCvZ5G\nr9/HgT8CnIDZfI1cu94WfT5+UKvVoQZ8/VJKW4CPAS/UXtuaUvr+INaqnhrJHgM6dsBzsAbA5Rra\nSEScADwI3FHryVIFRMQvAutrvZCB/y6r5EiLPqsCImIKeWh+EnAmcEJEvK3cqjSYBhqwXgLO6fb1\nxNprh77n7F7e059jNbgauX7UurcfBL6QUvrmINap3jVy/d4E/HJEPAs8APx8RHx+EGtVT41cuzXA\niymlBbXXHyQHLjVPI9dvDvDDlNLmlNIB4GvAzwxireqpkewxoGMHGrDmA+dHxKTaXRBvBQ69G+lb\nwDsAIuIN5O7Q9f08VoOrkesH8FngqZTS3zWrYPUw4OuXUvpgSumclHdmeCvwSErpHc0sfohr5Nqt\nB16MiKm1910DPNWkupU18rtzFfCGiBgZEUG+fiuaV/qQd7TZo3vv/oByS58rufcmpXQgIm4HHiaH\ntM+klFZExG352+nelNJ3I+L6iHgG2ElenPSIxw6kDg3MAK/fLQAR8Sbg14FlEbGYPI/ngymlh0r5\nYYagRv79qVwFXLv3AF+KiO6LPqtJGvzbt7TWW7wQOAAsBu4t5ycZevpz7Wo3IywATgQORsQdwEUp\npVcHkltcaFSSJKlgzZzkLkmSNCQYsCRJkgpmwJIkSSqYAUuSJKlgBixJkqSCGbAkSZIKZsCSJEkq\nmAFLkiSpYP8f27cUz58qJmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e86ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oov_train = inject_OOVs(train)\n",
    "oov_test = replace_OOVs(oov_vocab, test)\n",
    "bigram = NGramLM(oov_train,2)\n",
    "\n",
    "interval = [x/1000.0 for x in range(1, 100, 1)]\n",
    "perplexit_at_1 = perplexity(LaplaceLM(bigram, alpha=1.0), oov_test)\n",
    "\n",
    "def plot_perplexities(interval):\n",
    "    \"\"\"Plots the perplexity of LaplaceLM for every alpha in interval.\"\"\"\n",
    "    perplexities = [perplexity(LaplaceLM(bigram, alpha), oov_test) for alpha in interval]  # todo\n",
    "    plt.plot(interval, perplexities)\n",
    "    \n",
    "def find_optimal(low, high, epsilon=1e-6):\n",
    "    \"\"\"Returns the optimal pseudo count alpha within the interval [low, high] and the perplexity.\"\"\"\n",
    "    print(high, low)\n",
    "    if high - low < epsilon:\n",
    "        return high, perplexity(LaplaceLM(bigram, high), oov_test)\n",
    "    else:\n",
    "        mid = (high+low) / 2.0\n",
    "        left = perplexity(LaplaceLM(bigram, mid-epsilon), oov_test)\n",
    "        right = perplexity(LaplaceLM(bigram, mid+epsilon), oov_test)\n",
    "        if left < right:\n",
    "            return find_optimal(low, mid, epsilon)\n",
    "        else:\n",
    "            return find_optimal(mid, high, epsilon)\n",
    "\n",
    "plot_perplexities(interval)        \n",
    "find_optimal(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Sanity Check LM\n",
    "Implement a method that tests whether a language model provides a valid probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T16:57:55.397116",
     "start_time": "2016-10-21T16:57:55.328580"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0647115579930913\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "1.0647115579930913",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-869b4ff1682f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstupid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstupid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mOOV\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstupid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstupid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-869b4ff1682f>\u001b[0m in \u001b[0;36msanity_check\u001b[0;34m(lm, *history)\u001b[0m\n\u001b[1;32m      3\u001b[0m     in the vocabulary and UNK.\"\"\"  \n\u001b[1;32m      4\u001b[0m     \u001b[0mprobability_mass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_mass\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability_mass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0munigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moov_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 1.0647115579930913"
     ]
    }
   ],
   "source": [
    "def sanity_check(lm, *history):\n",
    "    \"\"\"Throws an AssertionError if lm does not define a valid probability distribution for all words \n",
    "    in the vocabulary and UNK.\"\"\"  \n",
    "    probability_mass = sum([lm.probability(word, *history) for word in lm.vocab])\n",
    "    assert abs(probability_mass - 1.0) < 1e-6, probability_mass\n",
    "\n",
    "unigram = NGramLM(oov_train,1)\n",
    "stupid = StupidBackoff(bigram, unigram, 0.1)\n",
    "print(sum([stupid.probability(word, 'the') for word in stupid.vocab]))\n",
    "assert OOV in stupid.vocab\n",
    "sanity_check(stupid, 'the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 3</font>: Subtract Count LM\n",
    "Develop and implement a language model that subtracts a count $d\\in[0,1]$ from each non-zero count in the training set. Let's first formalize this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\#_{w=0}(h_n) &= \\sum_{w \\in V} \\mathbf{1}[\\counts{\\train}{h_n,w} = 0]\\\\\n",
    "\\#_{w>0}(h_n) &= \\sum_{w \\in V} \\mathbf{1}[\\counts{\\train}{h_n,w} > 0]\\\\\n",
    "\\prob(w|h_n) &= \n",
    "\\begin{cases}\n",
    "\\frac{\\counts{\\train}{h_n,w} - d}{\\counts{\\train}{h_n}}  & \\mbox{if }\\counts{\\train}{h_n,w} > 0 \\\\\\\\\n",
    "\\frac{d*\\#_{w>0}(h_n)/\\#_{w=0}(h_n)}{\\counts{\\train}{h_n}} & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T16:58:08.278572",
     "start_time": "2016-10-21T16:57:58.756197"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.4414922652717"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SubtractCount(CountLM):        \n",
    "    def __init__(self, base_lm, d):\n",
    "        super().__init__(base_lm.vocab, base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        self.d = d            \n",
    "        self._counts = base_lm._counts  # not good style since it is a protected member\n",
    "        self.vocab = base_lm.vocab\n",
    "\n",
    "    def counts(self, word_and_history):\n",
    "        # todo: this can be chached and does not have to be called at every call of counts        \n",
    "        history = word_and_history[1:]        \n",
    "        num_non_zero_histories = len([x for x in self.vocab if self._counts[(x, ) + history] > 0])\n",
    "        num_zero_histories = len(self.vocab) - num_non_zero_histories        \n",
    "        if num_zero_histories == 0:\n",
    "            return self._counts[word_and_history]\n",
    "        else:        \n",
    "            if self._counts[word_and_history] > 0:\n",
    "                return self._counts[word_and_history] - self.d\n",
    "            else:            \n",
    "                return self.d * num_non_zero_histories / num_zero_histories\n",
    "\n",
    "    def norm(self, history):\n",
    "        return self.base_lm.norm(history)    \n",
    "    \n",
    "subtract_lm = SubtractCount(unigram, 0.1)\n",
    "oov_prob = subtract_lm.probability(UNK, 'the')\n",
    "rest_prob = sum([subtract_lm.probability(word, 'the') for word in subtract_lm.vocab])\n",
    "print(oov_prob + rest_prob)\n",
    "sanity_check(subtract_lm, 'the')\n",
    "perplexity(subtract_lm, oov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 4</font>: Normalisation of Stupid LM\n",
    "Develop and implement a version of the [stupid language model](https://github.com/uclmr/stat-nlp-book/blob/python/statnlpbook/lm.py#L205) that provides probabilities summing up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T16:58:08.402515",
     "start_time": "2016-10-21T16:58:08.280467"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.032236179798886"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StupidBackoffNormalized(LanguageModel):\n",
    "    def __init__(self, main, backoff, alpha):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.alpha = alpha               \n",
    "\n",
    "    def probability(self, word, *history):\n",
    "        main_counts = self.main.counts((word,)+tuple(history))\n",
    "        main_norm = self.main.norm(history)        \n",
    "        backoff_order_diff = self.main.order - self.backoff.order\n",
    "        backoff_counts = self.backoff.counts((word,)+tuple(history[:-backoff_order_diff]))\n",
    "        backoff_norm = self.backoff.norm(history[:-backoff_order_diff])        \n",
    "        counts = main_counts + self.alpha * backoff_counts\n",
    "        norm = main_norm + self.alpha * backoff_norm\n",
    "        return counts / norm\n",
    "        \n",
    "less_stupid = StupidBackoffNormalized(bigram, unigram, 0.1)\n",
    "print(sum([less_stupid.probability(word, 'the') for word in less_stupid.vocab]))\n",
    "sanity_check(less_stupid, 'the')\n",
    "perplexity(less_stupid, oov_test)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
