{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %cd .. \n",
    "import sys\n",
    "sys.path.append(\"../statnlpbook/\")\n",
    "#import statnlpbook.util as util\n",
    "#import statnlpbook.ie as ie\n",
    "import ie,tfutil\n",
    "#import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction \n",
    "Relation extraction (RE) is the task of extracting semantic relations between arguments. Arguments can either be general concepts such as \"a company\" (ORG), \"a person\" (PER); or instances of such concepts (e.g. \"Microsoft\", \"Bill Gates\"), which are called proper names or named entitites (NEs). An example for a semantic relation would be \"founder-of(PER, ORG)\". Relation extraction therefore often builds on the task of named entity recognition.\n",
    "\n",
    "Relation extraction is relevant for many high-level NLP tasks, such as\n",
    "\n",
    "* for question answering, where users ask questions such as \"Who founded Microsoft?\",\n",
    "* for information retrieval, which often relies on large collections of structured information as background data, and \n",
    "* for text and data mining, where larger patterns in relations between concepts are discovered, e.g. temporal patterns about startups\n",
    "\n",
    "\n",
    "## Relation Extraction as Structured Prediction\n",
    "We can formalise relation extraction as an instance of [structured prediction](/template/statnlpbook/02_methods/00_structuredprediction) where the input space $\\mathcal{X}$ are pairs of arguments $\\mathcal{E}$ and supporting texts $\\mathcal{S}$ those arguments appear in. The output space $\\mathcal{Y}$ is a set of relation labels such as $\\Ys=\\{ \\text{founder-of},\\text{employee-at},\\text{professor-at},\\text{NONE}\\}$. The goal is to define a model \\\\(s_{\\params}(\\x,y)\\\\) that assigns high *scores* to the label $\\mathcal{y}$ that fits the arguments and supporting text $\\mathcal{x}$, and lower scores otherwise. The model will be parametrized by \\\\(\\params\\\\), and these parameters we will learn from some training set \\\\(\\train\\\\) of $\\mathcal{x,y}$ pairs. When we need to classify input  instances $\\mathcal{x}$ consisting again of pairs of arguments and supporting texts, we have to solve the maximization problem $\\argmax_y s_{\\params}(\\x,y)$. Note that this frames relation extraction as a multi-class classification problem (Exercise: how could RE be formalised to predict multiple labels for each input instance and how would the example below have to be adapted for that?)\n",
    "\n",
    "\n",
    "## Relation Extraction Example\n",
    "Before we take a closer look at relation extraction methods, let us consider a concrete example. The concrete task we are considering here is to extract \"method used for task\" relations from sentences in computer science publications. As mentioned above, the first step would normally be to detection named entities, i.e. to determine tose pairs of arguments $\\mathcal{E}$. For simplicity, our training data already contains those annotations.\n",
    "\n",
    "\n",
    "## Pattern-Based Extraction\n",
    "The simplest relation extraction model defines a set of textual patterns for each relation and then assigns labels to entity pairs whose sentences match that pattern. The training data consists of entity pairs $\\mathcal{E}$, patterns $A$ and labels $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training patterns and entity pairs for relation 'method used for task'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('demonstrates XXXXX and clustering techniques for XXXXX',\n",
       "  ['text mining', 'building domain ontology']),\n",
       " ('demonstrates text mining and XXXXX for building XXXXX',\n",
       "  ['clustering techniques', 'domain ontology']),\n",
       " ('the XXXXX is able to enhance the XXXXX',\n",
       "  ['ensemble classifier', 'detection of construction materials']),\n",
       " ('we propose a fully XXXXX for 3d XXXXX of buildings',\n",
       "  ['autonomous system', 'thermal modeling']),\n",
       " ('this paper proposes two XXXXX to solve a XXXXX',\n",
       "  ['optimization models', 'dynamic supply chain issue'])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readLabelledPatternData(filepath=\"../data/ie/ie_bootstrap_patterns.txt\"):\n",
    "    f = open(filepath, \"r\")\n",
    "    patterns = []\n",
    "    entpairs = []\n",
    "    for l in f:\n",
    "        label, pattern, entpair = l.strip().replace(\"    \", \"\\t\").split(\"\\t\")\n",
    "        patterns.append(pattern)\n",
    "        entpair = entpair.strip(\"['\").strip(\"']\").split(\"', '\")\n",
    "        entpairs.append(entpair)\n",
    "    return patterns, entpairs\n",
    "\n",
    "training_patterns, training_entpairs = readLabelledPatternData()\n",
    "print(\"Training patterns and entity pairs for relation 'method used for task'\")\n",
    "[(tr_a, tr_e) for (tr_a, tr_e) in zip(training_patterns[:5], training_entpairs[:5])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patterns are currently sentences where the entity pairs where blanked with the placeholder 'XXXXX'.\n",
    "Note that for the training data, we also have labels. However, because we only have positive instances \n",
    "and only for one relation ('method used for task'), we do not differentiate between them. \n",
    "We read test data in the same way, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing patterns and entity pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a new combination of XXXXX and XXXXX is presented for optimal design of truss structures',\n",
       "  ['swarm intelligence', 'chaos theory']),\n",
       " ('a new combination of XXXXX and chaos theory is presented for XXXXX of truss structures',\n",
       "  ['swarm intelligence', 'optimal design']),\n",
       " ('a new combination of XXXXX and chaos theory is presented for optimal design of XXXXX',\n",
       "  ['swarm intelligence', 'truss structures']),\n",
       " ('a new combination of swarm intelligence and XXXXX is presented for XXXXX of truss structures',\n",
       "  ['chaos theory', 'optimal design']),\n",
       " ('a new combination of swarm intelligence and XXXXX is presented for optimal design of XXXXX',\n",
       "  ['chaos theory', 'truss structures'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readPatternData(filepath=\"../data/ie/ie_patterns.txt\"):\n",
    "    f = open(filepath, \"r\")\n",
    "    patterns = []\n",
    "    entpairs = []\n",
    "    for l in f:\n",
    "        pattern, entpair = l.strip().replace(\"    \", \"\\t\").split(\"\\t\")\n",
    "        patterns.append(pattern)\n",
    "        entpair = entpair.strip(\"['\").strip(\"']\").split(\"', '\")\n",
    "        entpairs.append(entpair)\n",
    "    return patterns, entpairs\n",
    "\n",
    "testing_patterns, testing_entpairs = readPatternData()\n",
    "print(\"Testing patterns and entity pairs\")\n",
    "[(tr_a, tr_e) for (tr_a, tr_e) in zip(testing_patterns[0:5], testing_entpairs[:5])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the testing data, we do not know the relations for the instances. We build a scoring model to determine which of the testing instances are examples for the relation 'method used for task' and which ones are not. (Thought exercise: how do we even know or can we ensure that even any of the instances here are examples of the relation in question?)\n",
    "\n",
    "A pattern scoring model \\\\(s_{\\params}(\\x,y)\\\\) only has one parameter and assignes scores to each relation label \\\\(y\\\\) proportional to the matches with the set of textual patterns. The final label assigned to each instance is then the one with the highest score.\n",
    "Here, our pattern scoring model is even simpler since we only have patterns for one relation. Hence the final label assigned to each instance is 'method used for task' if there is a match with a pattern, and 'NONE' if there is no match.\n",
    "\n",
    "Let's have a closer look at how pattern matching works now. Recall that the original patterns in the training data are sentences where the entity pairs are blanked with 'XXXXX'.\n",
    "\n",
    "We could use those patterns to find new sentences. However, we are not likely to find many since the patterns are very specific. Hence, we need to generalise those patterns to less specific ones. A simple way is to define the sequence of words between each entity pair as a pattern, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demonstrates XXXXX and clustering techniques for XXXXX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'and clustering techniques for'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentenceToShortPath(sent):\n",
    "    \"\"\"\n",
    "    Returns the path between two arguments in a sentence, where the arguments have been masked\n",
    "    Args:\n",
    "        sent: the sentence\n",
    "    Returns:\n",
    "        the path between to arguments\n",
    "    \"\"\"\n",
    "    sent_toks = sent.split(\" \")\n",
    "    indeces = [i for i, ltr in enumerate(sent_toks) if ltr == \"XXXXX\"]\n",
    "    pattern = \" \".join(sent_toks[indeces[0]+1:indeces[1]])\n",
    "    return pattern\n",
    "\n",
    "print(training_patterns[0])\n",
    "sentenceToShortPath(training_patterns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different alternatives to this. (Thought exercise: what are better ways of generalising patterns?)\n",
    "\n",
    "After the sentences shortening / pattern generalisation is defined, we can then apply those patterns to testing instances to classify them into 'method used for task' and 'NONE'. In practice, the code below returns only the instances which contain a 'method used for task' pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def patternExtraction(training_sentences, testing_sentences):\n",
    "    \"\"\"\n",
    "    Given a set of patterns for a relation, searches for those patterns in other sentences\n",
    "    Args:\n",
    "        sent: training sentences with arguments masked, testing sentences with arguments masked\n",
    "    Returns:\n",
    "        the testing sentences which the training patterns appeared in\n",
    "    \"\"\"\n",
    "    # convert training and testing sentences to short paths to obtain patterns\n",
    "    training_patterns = set([sentenceToShortPath(test_sent) for test_sent in training_sentences])\n",
    "    testing_patterns = [sentenceToShortPath(test_sent) for test_sent in testing_sentences]\n",
    "    # look for training patterns in testing patterns\n",
    "    testing_extractions = []\n",
    "    for i, testing_pattern in enumerate(testing_patterns):\n",
    "        if testing_pattern in training_patterns: # look for exact matches of patterns\n",
    "            testing_extractions.append(testing_sentences[i])\n",
    "    return testing_extractions\n",
    "\n",
    "patternExtraction(training_patterns[:500], testing_patterns[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Exercise: introduce patterns for other relations here and amend the scoring function in the Python code. Note that it is also possible to have 'NONE' patterns for 'no relation' between the entities.\n",
    "\n",
    "One of the shortcomings of this pattern-based approach is that the set of patterns has to be defined manually and the model does not learn new patterns. We will next look at an approach which addresses those two shortcomings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping relation extraction models take the same input as pattern-based approaches, i.e. a set of entity pairs and patterns. The overall idea is to extract more patterns and entity pairs iteratively. For this, we need two helper methods: one method that generalises from entity pairs to extract more patterns and entity pairs, and another one that generalises from patterns to extract more patterns and entity pairs.\n",
    "\n",
    "<!--Bootstrapping relation extraction models still take as input a set of entity pairs and patterns, same as pattern-based relation extraction approaches, but they aim at discovering new patterns.\n",
    "Algo:\n",
    "- Input: set of relation types \\\\(\\Ys\\\\), set of seed entity pairs \\\\(\\Es\\\\), set of seed patterns for each relation (\\Ps\\\\), set of sentences \\\\(\\Xs\\\\)\n",
    "- For each iteration\n",
    "    - Patterns P*\n",
    "    - Entity pairs E*\n",
    "    - For each sentence:\n",
    "        - if it contains a seed entity pair e:\n",
    "            - add the path between the entity pairs to P* as a new pattern\n",
    "        - if it contains a seed pattern p:\n",
    "            - identify an entity pair in the sentence and add it to E*\n",
    "    - P <- P + generalise(P*)\n",
    "    - E <- E + generalise(E*)\n",
    "We can examine the output of the model at each iteration-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchForPatternsAndEntpairsByPatterns(training_patterns, testing_patterns, testing_entpairs, testing_sentences):\n",
    "    testing_extractions = []\n",
    "    appearing_testing_patterns = []\n",
    "    appearing_testing_entpairs = []\n",
    "    for i, testing_pattern in enumerate(testing_patterns):\n",
    "        if testing_pattern in training_patterns: # if there is an exact match of a pattern\n",
    "            testing_extractions.append(testing_sentences[i])\n",
    "            appearing_testing_patterns.append(testing_pattern)\n",
    "            appearing_testing_entpairs.append(testing_entpairs[i])\n",
    "    return testing_extractions, appearing_testing_patterns, appearing_testing_entpairs\n",
    "\n",
    "\n",
    "def searchForPatternsAndEntpairsByEntpairs(training_entpairs, testing_patterns, testing_entpairs, testing_sentences):\n",
    "    testing_extractions = []\n",
    "    appearing_testing_patterns = []\n",
    "    appearing_testing_entpairs = []\n",
    "    for i, testing_entpair in enumerate(testing_entpairs):\n",
    "        if testing_entpair in training_entpairs: # if there is an exact match of an entity pair\n",
    "            testing_extractions.append(testing_sentences[i])\n",
    "            appearing_testing_entpairs.append(testing_entpair)\n",
    "            appearing_testing_patterns.append(testing_patterns[i])\n",
    "    return testing_extractions, appearing_testing_patterns, appearing_testing_entpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those two helper functions are then applied iteratively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'in'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ea2442694ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_extracts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtest_extracts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbootstrappingExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_entpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_entpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ea2442694ae4>\u001b[0m in \u001b[0;36mbootstrappingExtraction\u001b[0;34m(train_sents, train_entpairs, test_sents, test_entpairs, num_iter)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# convert training and testing sentences to short paths to obtain patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_patterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentenceToShortPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_patterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# too general, remove this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtest_patterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentenceToShortPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtest_extracts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'in'"
     ]
    }
   ],
   "source": [
    "def bootstrappingExtraction(train_sents, train_entpairs, test_sents, test_entpairs, num_iter):\n",
    "    \"\"\"\n",
    "    Given a set of patterns and entity pairs for a relation, extracts more patterns and entity pairs iteratively\n",
    "    Args:\n",
    "        train_sents: training sentences with arguments masked\n",
    "        train_entpairs: training entity pairs\n",
    "        test_sents: testing sentences with arguments masked\n",
    "        test_entpairs: testing entity pairs\n",
    "    Returns:\n",
    "        the testing sentences which the training patterns or any of the inferred patterns appeared in\n",
    "    \"\"\"\n",
    "\n",
    "    # convert training and testing sentences to short paths to obtain patterns\n",
    "    train_patterns = set([sentenceToShortPath(s) for s in train_sents])\n",
    "    train_patterns.remove(\"in\") # too general, remove this\n",
    "    test_patterns = [sentenceToShortPath(s) for s in test_sents]\n",
    "    test_extracts = []\n",
    "\n",
    "    # iteratively get more patterns and entity pairs\n",
    "    for i in range(1, num_iter):\n",
    "        print(\"Number extractions at iteration\", str(i), \":\", str(len(test_extracts)))\n",
    "        print(\"Number patterns at iteration\", str(i), \":\", str(len(train_patterns)))\n",
    "        print(\"Number entpairs at iteration\", str(i), \":\", str(len(train_entpairs)))\n",
    "        # get more patterns and entity pairs\n",
    "        test_extracts_p, ext_test_patterns_p, ext_test_entpairs_p = searchForPatternsAndEntpairsByPatterns(train_patterns, test_patterns, test_entpairs, test_sents)\n",
    "        test_extracts_e, ext_test_patterns_e, ext_test_entpairs_e = searchForPatternsAndEntpairsByEntpairs(train_entpairs, test_patterns, test_entpairs, test_sents)\n",
    "        # add them to the existing entity pairs for the next iteration\n",
    "        train_patterns.update(ext_test_patterns_p)\n",
    "        train_patterns.update(ext_test_patterns_e)\n",
    "        train_entpairs.extend(ext_test_entpairs_p)\n",
    "        train_entpairs.extend(ext_test_entpairs_e)\n",
    "        test_extracts.extend(test_extracts_p)\n",
    "        test_extracts.extend(test_extracts_e)\n",
    "\n",
    "    return test_extracts\n",
    "\n",
    "test_extracts = bootstrappingExtraction(training_patterns, training_entpairs, testing_patterns, testing_entpairs, num_iter=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that is noticable is that with each iteration, the number of extractions we find increases, but they are less correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_extracts[0:3])\n",
    "print(test_extracts[-4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons is that the semantics of the pattern shifts, so here we try to find new patterns for 'method used for task', but because the instances share a similar context with other relations, the patterns and entity pairs iteratively move away from the 'method used in task' relation. Another example in a different domain are the 'student-at' and 'lecturere-at' relations, that have many overlapping contexts.\n",
    "One way of improving this is with confidence values for each entity pair and pattern. For example, we might want to avoid patterns which are too general and penalise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "te_cnt = Counter()\n",
    "for te in test_extracts:\n",
    "    te_cnt[sentenceToShortPath(te)] += 1\n",
    "print(te_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that the 'in' pattern was found, which maches many contexts that are not 'method used for task'. (Exercise: implement a confidence weighting for patterns.)\n",
    "\n",
    "\n",
    "## Supervised Relation Extraction\n",
    "A different way of assigning a relation label to new instances is to follow the supervised learning paradigm, which we have already seen for other structured prediction tasks. For supervised relation extraction, the scoring model \\\\(s_{\\params}(\\x,y)\\\\) is estimated automatically based on training sentences $\\mathcal{X}$ and their labels $\\mathcal{Y}$.\n",
    "For the model, we can use range of different classifiers, e.g. a logistic regression model or an SVM. At testing time, the predict label for each testing instance is the highest-scoring one, i.e. $$ \\y^* = \\argmax_{\\y\\in\\Ys} s(\\x,\\y) $$\n",
    "\n",
    "First, we read in the training data, consisting again of patterns, entity pairs and labels. This time, the given labels for the training instances are 'method used for task' or 'NONE', i.e. we have positive and negative training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readLabelledData(filepath=\"../data/ie/ie_training_data.txt\"):\n",
    "    f = open(filepath, \"r\")\n",
    "    sents = []\n",
    "    entpairs = []\n",
    "    labels = []\n",
    "    for l in f:\n",
    "        label, sent, entpair = l.strip().replace(\"    \", \"\\t\").split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        sents.append(sent)\n",
    "        entpair = entpair.strip(\"['\").strip(\"']\").split(\"', '\")\n",
    "        entpairs.append(entpair)\n",
    "    return sents, entpairs, labels\n",
    "\n",
    "training_sents, training_entpairs, training_labels = readLabelledData()\n",
    "print(\"Manually labelled data set consists of\", training_labels.count(\"NONE\"), \n",
    "          \"negative training examples and\", training_labels.count(\"method used for task\"), \"positive training examples\")\n",
    "[(tr_s, tr_e, tr_l) for (tr_s, tr_e, tr_l) in zip(training_sents[:5], training_entpairs[:5], training_labels[:5])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define how to transform training and testing data to features. \n",
    "Features for the model are typically extracted from the shortest dependency path between two entities. Basic features are n-gram features, or they can be based on the syntactic structure of the input, i.e. the dependency path ([parsing](statnlpbook/chapters/parsing))\n",
    "Note that here we assume again that entity pairs are part of the input, i.e. we assume the named entity recognition problem to be solved as part of the preprocessing of the data. In reality, named entities have to be recognised first.\n",
    "\n",
    "Here, we use sklearn's built-in feature extractor which transforms sentences to n-grams with counts of their appearances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def featTransform(sents_train, sents_test):\n",
    "    cv = CountVectorizer()\n",
    "    cv.fit(sents_train)\n",
    "    print(cv.get_params())\n",
    "    features_train = cv.transform(sents_train)\n",
    "    features_test = cv.transform(sents_test)\n",
    "    return features_train, features_test, cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a model, again with sklearn, using one of their built-in classifiers and a prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model_train(feats_train, labels):\n",
    "    model = LogisticRegression(penalty='l2')  # logistic regression model with l2 regularisation\n",
    "    model.fit(feats_train, labels) # fit the model to the transformed training data\n",
    "    return model\n",
    "\n",
    "def predict(model, features_test):\n",
    "    \"\"\"Find the most compatible output class\"\"\"\n",
    "    preds = model.predict(features_test) # this returns the predicted labels\n",
    "    #preds_prob = model.predict_proba(features_test)  # this returns probablities instead of labels\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further define a helper function for debugging that determines the most useful features learned by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised relation extraction algorithm:\n",
    "\n",
    "<!--Algo:\n",
    " Transform to Python code \n",
    "- Input: set of training sentences \\\\(\\Xs\\\\) annotated with entity pairs \\\\(\\Es\\\\) and relation types \\\\(\\Ys\\\\) \n",
    "- features <- your_favourite_feature_extractor(training_sentences)\n",
    "- model <- train_model(features, labels)\n",
    "- predictions_test <- model(testing_sentences) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def supervisedExtraction(train_sents, train_entpairs, train_labels, test_sents, test_entpairs):\n",
    "    \"\"\"\n",
    "    Given pos/neg training instances, train a logistic regression model with simple BOW features and predict labels on unseen test instances\n",
    "    Args:\n",
    "        train_sents: training sentences with arguments masked\n",
    "        train_entpairs: training entity pairs\n",
    "        train_labels: labels of training instances\n",
    "        test_sents: testing sentences with arguments masked\n",
    "        test_entpairs: testing entity pairs\n",
    "    Returns:\n",
    "        predictions for the testing sentences\n",
    "    \"\"\"\n",
    "\n",
    "    # convert training and testing sentences to short paths to obtain patterns\n",
    "    train_patterns = [sentenceToShortPath(test_sent) for test_sent in train_sents]\n",
    "    test_patterns = [sentenceToShortPath(test_sent) for test_sent in test_sents]\n",
    "\n",
    "    # extract features\n",
    "    features_train, features_test, cv = featTransform(train_patterns, test_patterns)\n",
    "\n",
    "    # train model\n",
    "    model = model_train(features_train, train_labels)\n",
    "\n",
    "    # show most common features\n",
    "    show_most_informative_features(cv, model)\n",
    "\n",
    "    # get predictions\n",
    "    predictions = predict(model, features_test)\n",
    "\n",
    "    # show the predictions\n",
    "    for tup in zip(predictions, test_sents, test_entpairs):\n",
    "        print(tup)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "supervisedExtraction(training_sents, training_entpairs, training_labels, testing_patterns, testing_entpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of the features are common words (i.e. 'stop words', such as 'is') and very broad. Other features are very specific and thus might not appear very often. Typically these problems can be mitigated by using more sophisticated features such as those based on syntax (Exercise: use dependency parsing features or 3-gram features instead of bag of unigram features.) \n",
    "Also, the current model does not take into the entity pairs, only the path between the entity pairs. We will later examine a model that also takes entity pairs into account.\n",
    "Finally, the model requires manually annotated training data, which might not always be available. Next, we will look at a method that provides a solution for the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distant Supervision\n",
    "Supervised learning typically requires large amounts of hand-labelled training examples. Since it is time-consuming and expensive to manually label examples, it is desirable to find ways of automatically or semi-automatically producing more training data. We have already seen one example of this, bootstrapping.\n",
    "Although bootstrapping can be useful, one of the downsides already discussed above is semantic drift due to the iterative nature of finding good entity pairs and patterns. \n",
    "An alternative approach to this is to distant supervision. Here, we still have a set of entity pairs $\\mathcal{E}$, their relation types $\\mathcal{R}$ and a set of sentences $\\mathcal{X}$ as an input, but we do not require pre-defined patterns. Instead, a large number of such entity pairs and relations are obtained from a knowledge resource, e.g. the [Wikidata knowledge base](https://www.wikidata.org) or tables.\n",
    "These entity pairs and relations are then used to automatically label all sentences with relations if there exists an entity pair between which this relation holds according to the knowledge resource. After sentences are labelled in this way, the rest of the algorithm is the same the supervised relation extraction algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readDataForDistantSupervision(filepath=\"../data/ie/ie_training_data.txt\"):\n",
    "    f = open(filepath, \"r\")\n",
    "    unlab_sents = []\n",
    "    unlab_entpairs = []\n",
    "    kb_entpairs = []\n",
    "    for l in f:\n",
    "        label, sent, entpair = l.strip().replace(\"    \", \"\\t\").split(\"\\t\")\n",
    "        entpair = entpair.strip(\"['\").strip(\"']\").split(\"', '\")\n",
    "        # Define the positively labelled entity pairs as the KB ones, which are all for the same relation. \n",
    "        # Normally these would come from an actual KB.\n",
    "        if label != \"NONE\": \n",
    "            kb_entpairs.append(entpair)\n",
    "        unlab_sents.append(sent)\n",
    "        unlab_entpairs.append(entpair)\n",
    "    return kb_entpairs, unlab_sents, unlab_entpairs\n",
    "\n",
    "def distantlySupervisedLabelling(kb_entpairs, unlab_sents, unlab_entpairs):\n",
    "    \"\"\"\n",
    "    Label instances using distant supervision assumption\n",
    "    Args:\n",
    "        kb_entpairs: entity pairs for a specific relation\n",
    "        unlab_sents: unlabelled sentences with entity pairs anonymised\n",
    "        unlab_entpairs: entity pairs which were anonymised in unlab_sents\n",
    "\n",
    "    Returns: pos_train_sents, pos_train_enpairs, neg_train_sents, neg_train_entpairs\n",
    "\n",
    "    \"\"\"\n",
    "    train_sents, train_entpairs, train_labels = [], [], []\n",
    "    for i, unlab_entpair in enumerate(unlab_entpairs):\n",
    "        if unlab_entpair in kb_entpairs:  # if the entity pair is a KB tuple, it is a positive example for that relation\n",
    "            train_entpairs.append(unlab_entpair)\n",
    "            train_sents.append(unlab_sents[i])\n",
    "            train_labels.append(\"method used for task\")\n",
    "        else: # else, it is a negative example for that relation\n",
    "            train_entpairs.append(unlab_entpair)\n",
    "            train_sents.append(unlab_sents[i])\n",
    "            train_labels.append(\"NONE\")\n",
    "\n",
    "    return train_sents, train_entpairs, train_labels\n",
    "\n",
    "def distantlySupervisedExtraction(kb_entpairs, unlab_sents, unlab_entpairs, test_sents, test_entpairs):\n",
    "    # training_data <- Find training sentences with entity pairs\n",
    "    train_sents, train_entpairs, train_labels = distantlySupervisedLabelling(kb_entpairs, unlab_sents, unlab_entpairs)\n",
    "    \n",
    "    print(\"Distantly supervised labelling results in\", train_labels.count(\"NONE\"), \n",
    "          \"negative training examples and\", train_labels.count(\"method used for task\"), \"positive training examples\")\n",
    "    \n",
    "    # training works the same as for supervised RE\n",
    "    supervisedExtraction(train_sents, train_entpairs, train_labels, test_sents, test_entpairs) \n",
    "    \n",
    "kb_entpairs, unlab_sents, unlab_entpairs = readDataForDistantSupervision()\n",
    "print(len(kb_entpairs), \"'KB' entity pairs for relation 'method used for task' :\", kb_entpairs[0:5])\n",
    "print(len(unlab_entpairs), 'all entity pairs')\n",
    "distantlySupervisedExtraction(kb_entpairs, unlab_sents, unlab_entpairs, testing_patterns, testing_entpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we get here are the same as for supervised relation extraction. This is because the distant supervision heuristic identified the same positive and negative training examples as in the manually labelled dataset. In practice, the distant supervision heuristic typically leads to noisy training data due to several reasons.\n",
    "\n",
    "The first one is overlapping relations. For instance, 'employee-of' entails 'lecturer-at' and there are some overlapping entity pairs between the relations 'employee-of' and 'student-at'.\n",
    "\n",
    "The next problem is ambiguous entities, e.g. 'EM' has many possible meanings, only one of which is 'Expectation Maximisation', see [the Wikipedia disambiguation page for the acronym](https://en.wikipedia.org/wiki/EM).\n",
    "\n",
    "Next, not every sentence an entity pair that is a positive example for a relation appears in actually contains that relation, e.g. compare the sentence from [the Wikipedia EM definition](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) 'Expectationâ€“maximization algorithm, an algorithm for finding maximum likelihood estimates of parameters in probabilistic models' with 'In this section we introduce EM and probabilistic models'. The first one is a true mention of 'method used for task', whereas the second one is not.\n",
    "\n",
    "\n",
    "## Universal Schema\n",
    "Recall that for the pattern-based and bootstrapping approaches earlier, we were looking for simplified paths between entity pairs $\\mathcal{E}$ expressing a certain relation $\\mathcal{R}$ which we defined beforehand. This restricts the relation extraction problem to known relation types $\\mathcal{R}$. In order to overcome that limitation, we could have defined new relations on the spot and added them to $\\mathcal{R}$ by introducing new relation types for certain simplified paths between entity pairs.\n",
    "\n",
    "The goal of universal schemas is to overcome the limitation of having to pre-define relations, but within the supervised learning paradigm. This is possible by thinking of paths between entity pairs as relation expressions themselves. Simplified paths between entity pairs and relation labels are no longer considered separately, but instead the paths between entity pairs and relations is modelled in the same space. The space of entity pairs and relations is defined by a matrix:\n",
    "\n",
    "|  | demonstrates XXXXX for XXXXXX | XXXXX is capable of XXXXXX | an XXXXX model is employed for XXXXX | XXXXX decreases the XXXXX | method is used for task |\n",
    "| ------ | ----------- |\n",
    "| 'text mining', 'building domain ontology' | 1 |  |  |  | 1 |\n",
    "| 'ensemble classifier', 'detection of construction materials' |  |  | 1 |  | 1 |\n",
    "| 'data mining', 'characterization of wireless systems performance'|  | 1 |  |  | ? |\n",
    "| 'frequency domain', 'computational cost' |  |  |  | 1 | ? |\n",
    "\n",
    "Here, 'method is used for task' is a relation defined by a KB schema, whereas the other relations are surface pattern relations generated by blanking entity pairs in sentences. Where an entity pair and a KB relation or surface pattern relation co-occur, this is signified by a '1'. For some of the entities and surface pairs, a label for 'method used for task' is available, whereas for others, it is not (signified by the '?').\n",
    "The task is to turn the '?'s into 0/1 predictions for the 'method for task' relation.\n",
    "Note that this is the same data and task as for relation extraction extraction with supervised learning, merely the data representation is different. \n",
    "\n",
    "In order to solve this prediction task, we will learn to fill in the empty cells in the matrix. This is achieved by learning to distinguish between entity pairs and relations which co-occur in our training data and entity pairs and relations which are not known to co-occur (the empty cells).\n",
    "Each training instance consists of a surface pattern or KB relation $\\mathcal{r_{pos}}$ and an entity pair  $\\mathcal{e_{pos}}$ the relation co-occurs with, as well as a relation $\\mathcal{r_{neg}}$ and a entity pair $\\mathcal{e_{neg}}$ that do not co-occur in the training data. The positive relations and entity pairs are directly taken from the annotated data. The negative entity pairs and relations are sampled randomly from data points which are represented by the empty cell in the matrix above. The goal is to estimate, for a relation $\\mathcal{r}$ such as 'method is used for task' and an unseen entity pair such as $\\mathcal{e}$, e.g. ('frequency domain', 'computational cost'), what the probability $\\mathcal{p(y_{r,e} = 1)}$ is.\n",
    "\n",
    "First, we read in the annotated data and sample negative entity pairs and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# data reading\n",
    "training_sents, training_entpairs, training_labels = readLabelledData()\n",
    "\n",
    "# data converting\n",
    "def vectorise_data(training_sents, training_entpairs, training_kb_rels, testing_sents, testing_entpairs):\n",
    "\n",
    "    pos_train_ids, neg_train_ids = ie.split_labels_pos_neg(training_kb_rels + training_kb_rels)\n",
    "\n",
    "    training_toks_pos = [t.split(\" \") for i, t in enumerate(training_sents + training_kb_rels) if i in pos_train_ids]\n",
    "    training_toks_neg = [t.split(\" \") for i, t in enumerate(training_sents + training_kb_rels) if i in neg_train_ids]\n",
    "\n",
    "    training_ent_toks_pos = [\" || \".join(t).split(\" \") for i, t in enumerate(training_entpairs + training_entpairs) if i in pos_train_ids]\n",
    "    training_ent_toks_neg = [\" || \".join(t).split(\" \") for i, t in enumerate(training_entpairs + training_entpairs) if i in neg_train_ids]\n",
    "    testing_ent_toks = [\" || \".join(t).split(\" \") for t in testing_entpairs]\n",
    "\n",
    "    lens_rel = [len(s) for s in training_toks_pos + training_toks_neg]\n",
    "    lens_ents = [len(s) for s in training_ent_toks_pos + training_ent_toks_neg + testing_ent_toks]\n",
    "    print(\"Max relation length:\", max(lens_rel))\n",
    "    print(\"Max entity pair length:\", max(lens_ents))\n",
    "\n",
    "    count_rels, dictionary_rels, reverse_dictionary_rels = ie.build_dataset(\n",
    "        [token for senttoks in training_toks_pos + training_toks_neg for token in senttoks])\n",
    "\n",
    "    count_ents, dictionary_ents, reverse_dictionary_ents = ie.build_dataset(\n",
    "        [token for senttoks in training_ent_toks_pos + training_ent_toks_neg for token in senttoks])\n",
    "\n",
    "    rels_train_pos = [ie.transform_dict(dictionary_rels, senttoks, max(lens_rel)) for senttoks in training_toks_pos]\n",
    "    rels_train_neg = [ie.transform_dict(dictionary_rels, senttoks, max(lens_rel)) for senttoks in training_toks_neg]\n",
    "    ents_train_pos = [ie.transform_dict(dictionary_ents, senttoks, max(lens_ents)) for senttoks in training_ent_toks_pos]\n",
    "    ents_train_neg = [ie.transform_dict(dictionary_ents, senttoks, max(lens_ents)) for senttoks in training_ent_toks_neg]\n",
    "\n",
    "    # Negatively sample some entity pairs for training. Here we have some manually labelled neg ones, so we can sample from them.\n",
    "    ents_train_neg_samp = [random.choice(ents_train_neg) for _ in rels_train_neg]\n",
    "    \n",
    "    ents_test_pos = [ie.transform_dict(dictionary_ents, senttoks, max(lens_ents)) for senttoks in testing_ent_toks]\n",
    "    # Sample those test entity pairs from the training ones as for those we have neg annotations\n",
    "    ents_test_neg_samp = [random.choice(ents_train_neg) for _ in ents_test_pos]  \n",
    "\n",
    "    vocab_size_rels = len(dictionary_rels)\n",
    "    vocab_size_ents = len(dictionary_ents) \n",
    "\n",
    "    # for testing, we want to check if each unlabelled instance expresses the given relation \"method for task\"\n",
    "    rels_test_pos = [ie.transform_dict(dictionary_rels, training_toks_pos[-1], max(lens_rel)) for _ in testing_sents]\n",
    "    rels_test_neg_samp = [random.choice(rels_train_neg) for _ in rels_test_pos]\n",
    "\n",
    "    return rels_train_pos, rels_train_neg, ents_train_pos, ents_train_neg_samp, rels_test_pos, rels_test_neg_samp, \\\n",
    "           ents_test_pos, ents_test_neg_samp, vocab_size_rels, vocab_size_ents, max(lens_rel), max(lens_ents), \\\n",
    "           reverse_dictionary_rels, reverse_dictionary_ents\n",
    "        \n",
    "data = vectorise_data(training_sents, training_entpairs, training_labels, testing_patterns, testing_entpairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model entity pairs and relations through latent feature representations, $\\mathcal{v_e}$ and $\\mathcal{a_r}$, respectively. Each entity pair and relation thus corresponds to a vector, and we can measure the compatibility between them by taking their dot product, i.e. $\\mathcal{v_e * a_r}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model_f_reader(max_lens_rel, max_lens_ents, repr_dim, vocab_size_rels, vocab_size_ents):\n",
    "    \"\"\"\n",
    "    Create a Model F Universal Schema reader (Tensorflow graph).\n",
    "    Args:\n",
    "        max_rel_seq_length: maximum sentence sequence length\n",
    "        max_cand_seq_length: maximum candidate sequence length\n",
    "        repr_dim: dimensionality of vectors\n",
    "        vocab_size_rels: size of relation vocabulary\n",
    "        vocab_size_cands: size of candidate vocabulary\n",
    "    Returns:\n",
    "        dotprod_pos: dot product between positive entity pairs and relations\n",
    "        dotprod_neg: dot product between negative entity pairs and relations\n",
    "        diff_dotprod: difference in dot product of positive and negative instances, used for BPR loss (optional)\n",
    "        [relations_pos, relations_neg, ents_pos, ents_neg]: placeholders, fed in during training for each batch\n",
    "    \"\"\"\n",
    "    # Placeholders (empty Tensorflow variables) for positive and negative relations and entity pairs\n",
    "    # In each training epoch, for each batch, those will be set through mini batching\n",
    "\n",
    "    relations_pos = tf.placeholder(tf.int32, [None, max_lens_rel],\n",
    "                                   name='relations_pos')  # [batch_size, max_rel_seq_len]\n",
    "    relations_neg = tf.placeholder(tf.int32, [None, max_lens_rel],\n",
    "                                   name='relations_neg')  # [batch_size, max_rel_seq_len]\n",
    "\n",
    "    ents_pos = tf.placeholder(tf.int32, [None, max_lens_ents], name=\"ents_pos\")  # [batch_size, max_ent_seq_len]\n",
    "    ents_neg = tf.placeholder(tf.int32, [None, max_lens_ents], name=\"ents_neg\")  # [batch_size, max_ent_seq_len]\n",
    "\n",
    "    # Creating latent representations of relations and entity pairs\n",
    "    # latent feature representation of all relations, which are initialised randomly\n",
    "    relation_embeddings = tf.Variable(tf.random_uniform([vocab_size_rels, repr_dim], -0.1, 0.1, dtype=tf.float32),\n",
    "                                      name='rel_emb', trainable=True)\n",
    "\n",
    "    # latent feature representation of all entity pairs, which are initialised randomly\n",
    "    ent_embeddings = tf.Variable(tf.random_uniform([vocab_size_ents, repr_dim], -0.1, 0.1, dtype=tf.float32),\n",
    "                                 name='cand_emb', trainable=True)\n",
    "\n",
    "    # look up latent feature representation for relations and entities in current batch\n",
    "    rel_encodings_pos = tf.nn.embedding_lookup(relation_embeddings, relations_pos)\n",
    "    rel_encodings_neg = tf.nn.embedding_lookup(relation_embeddings, relations_neg)\n",
    "\n",
    "    ent_encodings_pos = tf.nn.embedding_lookup(ent_embeddings, ents_pos)\n",
    "    ent_encodings_neg = tf.nn.embedding_lookup(ent_embeddings, ents_neg)\n",
    "\n",
    "    # our feature representation here is a vector for each word in a relation or entity\n",
    "    # because our training data is so small\n",
    "    # we therefore take the sum of those vectors to get a representation of each relation or entity pair\n",
    "    rel_encodings_pos = tf.reduce_sum(rel_encodings_pos, 1)  # [batch_size, num_rel_toks, repr_dim]\n",
    "    rel_encodings_neg = tf.reduce_sum(rel_encodings_neg, 1)  # [batch_size, num_rel_toks, repr_dim]\n",
    "\n",
    "    ent_encodings_pos = tf.reduce_sum(ent_encodings_pos, 1)  # [batch_size, num_ent_toks, repr_dim]\n",
    "    ent_encodings_neg = tf.reduce_sum(ent_encodings_neg, 1)  # [batch_size, num_ent_toks, repr_dim]\n",
    "\n",
    "    # measuring compatibility between positive entity pairs and relations\n",
    "    # used for ranking test data\n",
    "    dotprod_pos = tf.reduce_sum(tf.mul(ent_encodings_pos, rel_encodings_pos), 1)\n",
    "\n",
    "    # measuring compatibility between negative entity pairs and relations\n",
    "    dotprod_neg = tf.reduce_sum(tf.mul(ent_encodings_neg, rel_encodings_neg), 1)\n",
    "\n",
    "    # difference in dot product of positive and negative instances\n",
    "    # used for BPR loss (ranking loss)\n",
    "    diff_dotprod = tf.reduce_sum(\n",
    "        tf.mul(ent_encodings_pos, rel_encodings_pos) - tf.mul(ent_encodings_neg, rel_encodings_neg), 1)\n",
    "\n",
    "    return dotprod_pos, dotprod_neg, diff_dotprod, [relations_pos, relations_neg, ents_pos, ents_neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model, we define a loss, which tries to maximise the distance between the positive and negative instances. One possibility of this is the logistic loss.\n",
    "\n",
    "$\\mathcal{\\sum -  log(v_{e_{pos}} * a_{r_{pos}})} + {\\sum log(v_{e_{neg}} * a_{r_{neg}}))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(1337)\n",
    "tf.set_random_seed(1337)\n",
    "\n",
    "def universalSchemaExtraction(data):\n",
    "    rels_train_pos, rels_train_neg, ents_train_pos, ents_train_neg_samp, rels_test_pos, rels_test_neg_samp, \\\n",
    "    ents_test_pos, ents_test_neg_samp, vocab_size_rels, vocab_size_ents, max_lens_rel, max_lens_ents, \\\n",
    "    dictionary_rels_rev, dictionary_ents_rev = data\n",
    "\n",
    "    batch_size = 4\n",
    "    repr_dim = 30\n",
    "    learning_rate = 0.001\n",
    "    max_epochs = 31\n",
    "\n",
    "    dotprod_pos, dotprod_neg, diff_dotprod, placeholders = create_model_f_reader(max_lens_rel, max_lens_ents, repr_dim, vocab_size_rels,\n",
    "                          vocab_size_ents)\n",
    "\n",
    "    # logistic loss\n",
    "    loss = tf.reduce_sum(tf.nn.softplus(-dotprod_pos)+tf.nn.softplus(dotprod_neg))\n",
    "\n",
    "    # alternative: BPR loss\n",
    "    #loss = tf.reduce_sum(tf.nn.softplus(diff_dotprod))\n",
    "\n",
    "    data = [np.asarray(rels_train_pos), np.asarray(rels_train_neg), np.asarray(ents_train_pos), np.asarray(ents_train_neg_samp)]\n",
    "    data_test = [np.asarray(rels_test_pos), np.asarray(rels_test_neg_samp), np.asarray(ents_test_pos), np.asarray(ents_test_neg_samp)]\n",
    "\n",
    "    # we use the Adam optimiser\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # training with mini-batches\n",
    "    batcher = tfutil.BatchBucketSampler(data, batch_size)\n",
    "    batcher_test = tfutil.BatchBucketSampler(data_test, 1, test=True)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        trainer = tfutil.Trainer(optimizer, max_epochs)\n",
    "        trainer(batcher=batcher, placeholders=placeholders, loss=loss, session=sess)\n",
    "\n",
    "        # we obtain test scores\n",
    "        test_scores = trainer.test(batcher=batcher_test, placeholders=placeholders, model=tf.nn.sigmoid(dotprod_pos), session=sess)\n",
    "\n",
    "    # show predictions\n",
    "    ents_test = [ie.reverse_dict_lookup(dictionary_ents_rev, e) for e in ents_test_pos]\n",
    "    rels_test = [ie.reverse_dict_lookup(dictionary_rels_rev, r) for r in rels_test_pos]\n",
    "    testresults = sorted(zip(test_scores, ents_test, rels_test), key=lambda t: t[0], reverse=True)  # sort for decreasing score\n",
    "\n",
    "    print(\"\\nTest predictions by decreasing probability:\")\n",
    "    for score, tup, rel in testresults:\n",
    "        print('%f\\t%s\\tREL\\t%s' % (score, \" \".join(tup), \" \".join(rel)))\n",
    "        \n",
    "        \n",
    "universalSchemaExtraction(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test predictions probabilities are obtained by scoring each test instances with:\n",
    "\n",
    "$\\mathcal{ \\sigma  ( v_{e} * a_{r} )}$\n",
    "\n",
    "Note that as input for the latent feature representation, we discarded words that only appeared twice. Hence, for those words we did not learn a representation, denoted here by 'UNK'. This is also typically done for other feature representations, as if we only see a feature once, it is difficult to learn weights for it.\n",
    "\n",
    "Exercises: \n",
    "The scores shown here are for the relation 'method used for task'. However, we could also use our model to score the compatibility of entity pairs with other relations, e.g. 'demonstrates XXXXX for XXXXXX'. How could this be done here?\n",
    "How could we get around the problem of unseen words, as described above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Background\n",
    "Jurafky, Dan and Martin, James H. (2016). Speech and Language Processing, Chapter 21 (Information Extraction): https://web.stanford.edu/~jurafsky/slp3/21.pdf\n",
    "\n",
    "Riedel, Sebastian and Yao, Limin and McCallum, Andrew and Marlin, Benjamin M. (2013). Extraction with Matrix Factorization and Universal Schemas. Proceedings of NAACL.  http://www.aclweb.org/anthology/N13-1008"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
