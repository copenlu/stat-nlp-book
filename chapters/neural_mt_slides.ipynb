{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n",
       "<style>\n",
       ".rendered_html td {\n",
       "    font-size: xx-large;\n",
       "    text-align: left; !important\n",
       "}\n",
       ".rendered_html th {\n",
       "    font-size: xx-large;\n",
       "    text-align: left; !important\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n",
    "<style>\n",
    ".rendered_html td {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    ".rendered_html th {\n",
    "    font-size: xx-large;\n",
    "    text-align: left; !important\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import statnlpbook.util as util\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Schedule\n",
    "\n",
    "+ History: machine translation (5 min.)\n",
    "\n",
    "+ Background: neural MT (12 min.)\n",
    "\n",
    "+ Example: beam search (13 min.)\n",
    "\n",
    "+ Math: BLEU (5 min.)\n",
    "\n",
    "+ Exercise: debugging NMT (10 min.)\n",
    "\n",
    "+ Break (10 min.)\n",
    "\n",
    "+ Attention ([slides](attention_slides.ipynb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Languages are hard (even for humans)!\n",
    "\n",
    "<center style=\"padding-top:3em;\">\n",
    "  <img src=\"mt_figures/whatever.jpg\" />\n",
    "    \n",
    "  <span style=\"font-size:50%;\">(Source: <a href=\"https://www.flickr.com/photos/98991392@N00/8729849093/sizes/z/in/pool-47169589@N00/\">Flickr</a>)</span>\n",
    "</center>\n",
    "\n",
    "[随便](https://translate.google.com/#view=home&op=translate&sl=zh-CN&tl=en&text=%E9%9A%8F%E4%BE%BF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Automatic machine translation is hard!\n",
    "\n",
    "<center style=\"padding-top:3em;\">\n",
    "<img src=\"../chapters/mt_figures/avocado.png\" width=\"100%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to implement?\n",
    "\n",
    "<center>\n",
    "   <img src=\"mt_figures/brief_history.png\" width=\"100%\" />\n",
    "   \n",
    "   <span style=\"font-size:50%;\">(Source: <a href=\"https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/\">freeCodeCamp</a>)</span>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to implement?\n",
    "\n",
    "* Four paradigms:\n",
    "  * Rule-based machine translation (RBMT)\n",
    "  * Example-based machine translation (EBMT)\n",
    "  * Statistical machine translation (SMT)\n",
    "  * Neural machine translation (NMT)\n",
    "    &nbsp;\n",
    "\n",
    "* An informal but entertaining overview: [\"A history of machine translation from the Cold War to deep learning\"](https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/)\n",
    "\n",
    "* Details about SMT in [previous slides from this repo](word_mt_slides.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "Sequence-to-sequence model (seq2seq), encoder–decoder architecture\n",
    "([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf))\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/encdec.svg\" width=\"70%\" />\n",
    "</center>\n",
    "\n",
    "(Examples are Basque–English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We can use RNNs for that!\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/encdec_rnn1.svg\" width=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "+ Word embedding layer\n",
    "+ **Bi-directional LSTM** to best capture contextual information\n",
    "\n",
    "### Decoder\n",
    "\n",
    "+ **Uni-directional LSTM** — because we need to *decode* from left to right!\n",
    "+ Softmax layer on top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### But something's missing (again)...\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/encdec_rnn1.svg\" width=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many things could go wrong.\n",
    "\n",
    "### https://www.translationparty.com/whatever-13376386\n",
    "\n",
    "(https://tinyurl.com/gtrans-egu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output words **depend on each other**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The input-feeding approach\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    <img src=\"mt_figures/encdec_rnn2.svg\" width=\"70%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is happening here exactly?\n",
    "\n",
    "+ We added an **embedding layer** to the decoder\n",
    "+ At the first timestep, we use a special **start symbol** (here: `<S>`) as input\n",
    "+ We **feed the predictions** back into the next timestep:\n",
    "\n",
    "  1. The softmax layer gives us a probability distribution over words\n",
    "  2. We pick the most likely one *(greedy decoding)*\n",
    "  3. We use its embedding as input for the decoder's next timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the model\n",
    "\n",
    "+ Loss function: negative log-likelihood\n",
    "  - Just as before!  Words ~ Labels\n",
    "    &nbsp;\n",
    "\n",
    "+ **Teacher forcing:**  always feed the ground truth into the decoder\n",
    "\n",
    "*Alternative:*\n",
    "\n",
    "+ **Scheduled sampling:** with a certain probability, use model predictions instead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the model\n",
    "\n",
    "+ **Vocabulary size** becomes a problem\n",
    "  $\\rightarrow$ Have to do softmax over *all words in the target language!*\n",
    "  &nbsp;\n",
    "\n",
    "+ Solution 1: **restrict the vocabulary**\n",
    "  - Use only $n$ most frequent words (e.g., $n = 50000$)\n",
    "  - Replace all less frequent words with `<UNK>` symbol\n",
    "  &nbsp;\n",
    "\n",
    "+ Solution 2: **subword tokenization** (see [§13.6.2 in Koehn](https://arxiv.org/pdf/1709.07809.pdf))\n",
    "  - Byte-pair encoding (BPE)\n",
    "  - Character-based decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why do we need to handle decoding?\n",
    "\n",
    "$\\rightarrow$ *We don't know how long the output sequence is going to be!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Special symbol to denote *end of sequence*\n",
    "+ For example: `</S>`, `<EOS>`, `<END>`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Greedy decoding\n",
    "\n",
    "For machine translation:\n",
    "\n",
    "+ Always pick the **most likely word** (according to the softmax)\n",
    "+ Continue generating more words **until the `</S>` symbol is predicted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Beam search decoding\n",
    "\n",
    "Recall:\n",
    "\n",
    "+ Greedy decoding may lead to **search errors** when returned $\\y$ is not highest scoring **global** solution\n",
    "\n",
    "+ With input feeding, future predictions will change based on previous ones\n",
    "\n",
    "+ Exhaustive search not feasible\n",
    "\n",
    "\n",
    "Popular solution:\n",
    "\n",
    "+ **Beam search** as an *approximate search strategy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beam search algorithm (again)\n",
    "\n",
    "Keep a \"beam\" of the best $\\beta$ previous solutions\n",
    "\n",
    "1. Choose $\\beta$ highest scoring labels for token 1\n",
    "2. 1. For each of the previous $\\beta$ labels: predict probabilities for next label, conditioned on the previous label(s)\n",
    "   2. **Sum** the log-likelihoods for previous states and next label\n",
    "   3. **Prune** the beam by only keeping the top $\\beta$ paths\n",
    "3. Repeat until **all items reach** end of sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example\n",
    "\n",
    "Beam size $\\beta = 3$\n",
    "\n",
    "| $y_0$ | $y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle )$ |\n",
    "|-|-|-|\n",
    "| &lt;S&gt; | I | -1.670 |\n",
    "| &lt;S&gt; | We | -3.266 |\n",
    "| &lt;S&gt; | He | -3.364 |\n",
    "| &lt;S&gt; | She | -3.366 |\n",
    "| &lt;S&gt; | They | -4.920 |\n",
    "| &lt;S&gt; | ... | ... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example\n",
    "\n",
    "Beam size $\\beta = 3$\n",
    "\n",
    "| $y_0$ | $y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle )$ |\n",
    "|-|-|-|\n",
    "| **&lt;S&gt;** | **I** | **-1.670** |\n",
    "| **&lt;S&gt;** | **We** | **-3.266** |\n",
    "| **&lt;S&gt;** | **He** | **-3.364** |\n",
    "| &lt;S&gt; | She | -3.366 |\n",
    "| &lt;S&gt; | They | -4.920 |\n",
    "| &lt;S&gt; | ... | ... |\n",
    "\n",
    "1. Choose $\\beta$ highest scoring labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example\n",
    "\n",
    "Beam size $\\beta = 3$\n",
    "\n",
    "### Our beam\n",
    "\n",
    "| $y_0,y_1$ | $\\log p_\\theta(y_0,y_1|\\x)$ |\n",
    "|-|-|\n",
    "| &lt;S&gt; I | -1.670 |\n",
    "| &lt;S&gt; We | -3.266 |\n",
    "| &lt;S&gt; He | -3.364 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example\n",
    "\n",
    "Beam size $\\beta = 3$\n",
    "\n",
    "| $y_0,y_1$ | $y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle \\text{ I})$ | | $y_0,y_1$ | $y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle \\text{ We})$ | | $y_0,y_1$ | $y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle \\text{ He})$ |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|\n",
    "| &lt;S&gt; I | love | -1.141 | | &lt;S&gt; We | love | -1.129 | | &lt;S&gt; He | loves | -2.916 |\n",
    "| &lt;S&gt; I | like | -1.673 | | &lt;S&gt; We | like | -2.367 | | &lt;S&gt; He | will  | -4.267 |\n",
    "| &lt;S&gt; I | enjoy | -3.906 | | &lt;S&gt; We | have | -2.904 | | &lt;S&gt; He | likes | -4.619 |\n",
    "| &lt;S&gt; I | have | -4.366 | | &lt;S&gt; We | enjoy | -4.148 | | &lt;S&gt; He | loved | -5.698 |\n",
    "| &lt;S&gt; I | ... | ... | | &lt;S&gt; We | ... | ... | | &lt;S&gt; He | ... | ... |\n",
    "\n",
    "2. A. For each of the previous $\\beta$ labels: predict probabilities for next label, conditioned on the previous label(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example\n",
    "\n",
    "Beam size $\\beta = 3$\n",
    "\n",
    "| $y_0,y_1$ | $y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle \\text{ I})-1.670$ | | $y_0,y_1$ | $\\y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle \\text{ We})-3.266$ | | $y_0,y_1$ | $\\y$ | $\\log p_\\theta(y|\\x,\\langle S \\rangle \\text{ He})-3.364$ |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|\n",
    "| &lt;S&gt; I | love | -2.811 | | &lt;S&gt; We | love | -4.395 | | &lt;S&gt; He | loves | -6.280 |\n",
    "| &lt;S&gt; I | like | -3.343 | | &lt;S&gt; We | like | -5.633 | | &lt;S&gt; He | will  | -7.631 |\n",
    "| &lt;S&gt; I | enjoy | -5.576 | | &lt;S&gt; We | have | -6.170 | | &lt;S&gt; He | likes | -7.983 |\n",
    "| &lt;S&gt; I | have | -6.036 | | &lt;S&gt; We | enjoy | -7.414 | | &lt;S&gt; He | loved | -9.062 |\n",
    "| &lt;S&gt; I | ... | ... | | &lt;S&gt; We | ... | ... | | &lt;S&gt; He | ... | ... |\n",
    "\n",
    "2. B. **Sum** the log-likelihoods for previous states and next label\n",
    "\n",
    "because $\\log p_\\theta(y_0,y_1,y|\\x) = \\log p_\\theta(y|\\x,y_0, y_1) + \\log p_\\theta(y_0,y_1|\\x)$ (Bayes' rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example\n",
    "\n",
    "Beam size $\\beta = 3$\n",
    "\n",
    "| $y_0,y_1$ | $y$ | $\\log p_\\theta(\\y|\\x)$ | | $y_0$ | $\\y$ | $\\log p_\\theta(\\y|\\x)$ | | $y_0$ | $\\y$ | $\\log p_\\theta(\\y|\\x)$ |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|\n",
    "| **&lt;S&gt; I** | **love** | **-2.811** | | **&lt;S&gt; We** | **love** | **-4.395** | | &lt;S&gt; He | loves | -6.280 |\n",
    "| **&lt;S&gt; I** | **like** | **-3.343** | | &lt;S&gt; We | like | -5.633 | | &lt;S&gt; He | will  | -7.631 |\n",
    "| &lt;S&gt; I | enjoy | -5.576 | | &lt;S&gt; We | have | -6.170 | | &lt;S&gt; He | likes | -7.983 |\n",
    "| &lt;S&gt; I | have | -6.036 | | &lt;S&gt; We | enjoy | -7.414 | | &lt;S&gt; He | loved | -9.062 |\n",
    "| &lt;S&gt; I | ... | ... | | &lt;S&gt; We | ... | ... | | &lt;S&gt; He | ... | ... |\n",
    "\n",
    "2. C. **Prune** the beam by only keeping the top $\\beta$ paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example\n",
    "\n",
    "Beam size $\\beta = 3$\n",
    "\n",
    "### Our beam\n",
    "\n",
    "| $\\y$ | $\\log p_\\theta(\\y|\\x)$ |\n",
    "|-|-|\n",
    "| &lt;S&gt; I love | -2.811 |\n",
    "| &lt;S&gt; I like | -3.343 |\n",
    "| &lt;S&gt; We love | -4.395 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Continue until all hypotheses have generated `</S>`\n",
    "+ Typically: normalize by length of hypothesis to find final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We're training the model with *negative log-likelihood*, but that's not the best way to *evaluate* it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider:\n",
    "    \n",
    "+ After lunch, he went to the gym.\n",
    "+ After he had lunch, he went to the gym.\n",
    "+ He went to the gym after lunch.\n",
    "+ He went to the gym after lunchtime.\n",
    "\n",
    "In machine translation, there are often **several acceptable variations!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BLEU score\n",
    "\n",
    "A widely used, but simplistic, metric:\n",
    "\n",
    "+ Compare the prediction to one or more reference translations.\n",
    "+ Count the number of matching $n$-grams between them.\n",
    "  - It is common to consider $1 \\le n \\le 4$\n",
    "+ Divide by total number of $n$-grams.\n",
    "\n",
    "The BLEU score will range between 0 (*no match at all*) and 1.0 (*perfect match*, 100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BLEU score examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "refs = [[\"After\", \"lunch\", \",\", \"he\", \"went\", \"to\", \"the\", \"gym\", \".\"],\n",
    "        [\"He\", \"went\", \"to\", \"the\", \"gym\", \"after\", \"lunch\", \".\"]]\n",
    "\n",
    "sentence_bleu(refs, [\"After\", \"lunch\", \",\", \"he\", \"went\", \"to\", \"the\", \"gym\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence_bleu(refs, [\"Turtles\", \"are\", \"great\", \"animals\", \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BLEU score examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence_bleu(refs, [\"After\", \"he\", \"had\", \"lunch\", \",\", \"he\", \"went\", \"to\", \"the\", \"gym\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence_bleu(refs, [\"After\", \"lunch\", \",\", \"he\", \"went\", \"to\", \"the\", \"pizzeria\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence_bleu(refs, [\"Before\", \"lunch\", \",\", \"he\", \"went\", \"to\", \"the\", \"gym\", \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ BLEU is very simplistic\n",
    "+ Many alternatives have been proposed\n",
    "+ ...but BLEU still remains the de-facto standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "+ **Encoder–decoder** architecture with (bi-)LSTMs\n",
    "  - Input feeding\n",
    "  - Teacher forcing\n",
    "t\n",
    "+ **Beam search** vs. greedy decoding\n",
    "\n",
    "+ Evaluation with **BLEU**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outlook\n",
    "\n",
    "Competitive machine translation models are *very time-intensive* to train!\n",
    "\n",
    "+ Example: [Wu et al. (2016)](https://arxiv.org/pdf/1609.08144.pdf) describe Google's NMT system\n",
    "\n",
    "+ Encoder–decoder with attention & stack of 8 LSTM layers\n",
    "  (plus some other additions)\n",
    "  \n",
    "+ 36 million sentence pairs for English-to-French setting (En→Fr)\n",
    "\n",
    "Quote:\n",
    "\n",
    "> On WMT En→Fr, it takes around 6 days to train a basic model using 96 NVIDIA K80 GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From a [paper recently submitted to ICLR 2020](https://openreview.net/pdf?id=Bkl8YR4YDB):\n",
    "\n",
    "+ Setting is training NMT on Chinese–English with 50 **billion** sentence pairs\n",
    "\n",
    "> We use a single transformer model to fit all the training data.  We use 512 Nvidia V100 GPUs with mini-batches of approximately 1M tokens. [...] Upon the submission of this paper, training has lasted for three months, 2 epochs in total, and perplexity on the development set is still dropping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* Non-neural machine translation:\n",
    "  + Ilya Pestov's article [A history of machine translation from the Cold War to deep learning](https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/)\n",
    "  + [Slides on SMT from this repo](word_mt_slides.ipynb)\n",
    "  + Mike Collins's [Lecture notes on IBM Model 1 and 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf)\n",
    "\n",
    "* Sequence-to-sequence models:\n",
    "  + Graham Neubig, [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/abs/1703.01619)\n",
    "\n",
    "* And beyond...\n",
    "  + Philipp Koehn, [Neural Machine Translation, §13.6–13.8](https://arxiv.org/abs/1709.07809) gives a great overview of further refinements and challenges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 1 -f svg --save mt_figures/encdec_rnn1.svg\n",
    "#\n",
    "#  \\tikzset{state/.style={draw,rectangle,minimum height=1.5em,minimum width=2em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em},\n",
    "#            addstate/.style={draw,circle,inner sep=0.1em,fill=gray!10},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.2em},\n",
    "#            encembed/.style={fill=green!40!gray!40},\n",
    "#            decembed/.style={fill=blue!40!gray!40},\n",
    "#            encoder/.style={fill=green!40!gray!40},\n",
    "#            decoder/.style={fill=blue!40!gray!40},\n",
    "#            outer/.style={outer sep=0},\n",
    "#            label/.style={align=center,font=\\bfseries\\small\\sffamily,text height=0.5em}}\n",
    "#\n",
    "#      % input labels\n",
    "#      \\foreach \\i [count=\\step from 1] in {Musika,maite,dut,{$<$/S$>$}} {\n",
    "#        \\node[emptystate]    (EncI\\step)  at (1.5*\\step-1.5, 0)  {\\i};\n",
    "#      }\n",
    "#\n",
    "#      % embedding layers\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,encembed] (EncE\\step)  at (1.5*\\step-1.5, 1.5) {};\n",
    "#        \\draw[->]             (EncI\\step)  to (EncE\\step);\n",
    "#      }\n",
    "#\n",
    "#      % encoder LSTMs\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,encoder] (EncLA\\step)  at (1.5*\\step-1.5, 2.5) {};\n",
    "#        \\draw[->]            (EncE\\step)  to (EncLA\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.east] (EncLA_be\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.west] (EncLA_bw\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.east] (EncLA_ae\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.west] (EncLA_aw\\step);\n",
    "#      }\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\draw[densely dashed, ->]   (EncLA_be\\step) to (EncLA_bw\\next);\n",
    "#        \\draw[densely dashed, ->]   (EncLA_aw\\next) to (EncLA_ae\\step);\n",
    "#      }\n",
    "#\n",
    "#      % encoded vectors\n",
    "#      \\node[addstate]        (EncVecA)  at (6.0, 2.5)   {$\\oplus$};\n",
    "#      \\draw[densely dashed, ->]     (EncLA_be4) to (EncVecA);\n",
    "#      \\draw[densely dashed, ->, rounded corners=5pt]     (EncLA_aw1) -|([shift={(-5mm,3mm)}]EncLA1.north west) -- ([shift={(-10mm,4.2mm)}]EncVecA.north west) to (EncVecA.north west);\n",
    "#\n",
    "#\n",
    "#      % decoder LSTMs\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,decoder] (DecLA\\step)  at (1.5*\\step+6.0, 2.5) {};\n",
    "#      }\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\draw[densely dashed, ->]   (DecLA\\step.east) to (DecLA\\next.west);\n",
    "#      }\n",
    "#      \\draw[densely dashed, ->]     (EncVecA.east)    to (DecLA1.west);\n",
    "#\n",
    "#      % dense layer\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,decoder] (DecD\\step)  at (1.5*\\step+6.0, 3.5) {};\n",
    "#        \\draw[->]            (DecLA\\step) to (DecD\\step);\n",
    "#      }\n",
    "#\n",
    "#      % output labels\n",
    "#      \\foreach \\i [count=\\step from 1] in {I,love,music,{$<$/S$>$}} {\n",
    "#        \\node[emptystate]    (DecO\\step)  at (1.5*\\step+6.0, 5.0)  {\\i};\n",
    "#        \\draw[->]            (DecD\\step)  to (DecO\\step);\n",
    "#      }\n",
    "#\n",
    "#      % figure labels\n",
    "#      \\node[label,text=blue!40!black!60]   (DecLabel)   at (5.25, 3.75)  {(Uni-)LSTM Decoder};\n",
    "#      \\node[label,text=green!40!black!60]  (EncLabel)   at (1.0, 3.75)  {Bi-LSTM Encoder};\n",
    "#      \\node[label,text=black!60]          (asd)   at (6.0, 1.5)  {Sentence\\\\Vector};\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 1 -f svg --save mt_figures/encdec_rnn2.svg\n",
    "#\n",
    "#  \\tikzset{state/.style={draw,rectangle,minimum height=1.5em,minimum width=2em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em},\n",
    "#            addstate/.style={draw,circle,inner sep=0.1em,fill=gray!10},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.2em},\n",
    "#            encembed/.style={fill=green!40!gray!40},\n",
    "#            decembed/.style={fill=blue!40!gray!40},\n",
    "#            encoder/.style={fill=green!40!gray!40},\n",
    "#            decoder/.style={fill=blue!40!gray!40},\n",
    "#            outer/.style={outer sep=0},\n",
    "#            label/.style={align=center,font=\\bfseries\\small\\sffamily,text height=0.5em}}\n",
    "#\n",
    "#      % input labels\n",
    "#      \\foreach \\i [count=\\step from 1] in {Musika,maite,dut,{$<$/S$>$}} {\n",
    "#        \\node[emptystate]    (EncI\\step)  at (1.5*\\step-1.5, 0)  {\\i};\n",
    "#      }\n",
    "#      \\foreach \\i [count=\\step from 1] in {{$<$S$>$},I,love,music} {\n",
    "#        \\node[emptystate]    (DecI\\step)  at (1.5*\\step+6.0, 0)  {\\i};\n",
    "#      }\n",
    "#\n",
    "#      % embedding layers\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,encembed] (EncE\\step)  at (1.5*\\step-1.5, 1.5) {};\n",
    "#        \\node[state,decembed] (DecE\\step)  at (1.5*\\step+6.0, 1.5) {};\n",
    "#        \\draw[->]             (EncI\\step)  to (EncE\\step);\n",
    "#        \\draw[->]             (DecI\\step)  to (DecE\\step);\n",
    "#      }\n",
    "#\n",
    "#      % encoder LSTMs\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,encoder] (EncLA\\step)  at (1.5*\\step-1.5, 2.5) {};\n",
    "#        \\draw[->]            (EncE\\step)  to (EncLA\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.east] (EncLA_be\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.west] (EncLA_bw\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.east] (EncLA_ae\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.west] (EncLA_aw\\step);\n",
    "#      }\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\draw[densely dashed, ->]   (EncLA_be\\step) to (EncLA_bw\\next);\n",
    "#        \\draw[densely dashed, ->]   (EncLA_aw\\next) to (EncLA_ae\\step);\n",
    "#      }\n",
    "#\n",
    "#      % encoded vectors\n",
    "#      \\node[addstate]        (EncVecA)  at (6.0, 2.5)   {$\\oplus$};\n",
    "#      \\draw[densely dashed, ->]     (EncLA_be4) to (EncVecA);\n",
    "#      \\draw[densely dashed, ->, rounded corners=5pt]     (EncLA_aw1) -|([shift={(-5mm,3mm)}]EncLA1.north west) -- ([shift={(-10mm,4.2mm)}]EncVecA.north west) to (EncVecA.north west);\n",
    "#\n",
    "#\n",
    "#      % decoder LSTMs\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,decoder] (DecLA\\step)  at (1.5*\\step+6.0, 2.5) {};\n",
    "#        \\draw[->]            (DecE\\step)  to (DecLA\\step);\n",
    "#      }\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\draw[densely dashed, ->]   (DecLA\\step.east) to (DecLA\\next.west);\n",
    "#      }\n",
    "#      \\draw[densely dashed, ->]     (EncVecA.east)    to (DecLA1.west);\n",
    "#\n",
    "#      % dense layer\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,decoder] (DecD\\step)  at (1.5*\\step+6.0, 3.5) {};\n",
    "#        \\draw[->]            (DecLA\\step) to (DecD\\step);\n",
    "#      }\n",
    "#\n",
    "#      % output labels\n",
    "#      \\foreach \\i [count=\\step from 1] in {I,love,music,{$<$/S$>$}} {\n",
    "#        \\node[emptystate]    (DecO\\step)  at (1.5*\\step+6.0, 5.0)  {\\i};\n",
    "#        \\draw[->]            (DecD\\step)  to (DecO\\step);\n",
    "#      }\n",
    "#\n",
    "#      % input-feeding connections\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\node[emptystate]   (Midway\\step) at (1.5*\\step+6.75, 3.75) {};\n",
    "#        \\draw[densely dotted, ->, rounded corners=2pt]    (DecO\\step.east) -|(1.5*\\step+6.75, 3.75) |-(DecI\\next.west);\n",
    "#      }\n",
    "#\n",
    "#      % figure labels\n",
    "#      \\node[label,text=blue!40!black!60]   (DecLabel)   at (5.25, 3.75)  {(Uni-)LSTM Decoder};\n",
    "#      \\node[label,text=green!40!black!60]  (EncLabel)   at (1.0, 3.75)  {Bi-LSTM Encoder};\n",
    "#      \\node[label,text=black!60]          (asd)   at (6.0, 1.5)  {Sentence\\\\Vector};\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 1 -f svg --save mt_figures/encdec_rnn3.svg\n",
    "#\n",
    "#  \\tikzset{state/.style={draw,rectangle,minimum height=1.5em,minimum width=2em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em},\n",
    "#            addstate/.style={draw,circle,inner sep=0.1em,fill=gray!10},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.2em},\n",
    "#            encembed/.style={fill=green!40!gray!40},\n",
    "#            decembed/.style={fill=blue!40!gray!40},\n",
    "#            encoder/.style={fill=green!40!gray!40},\n",
    "#            decoder/.style={fill=blue!40!gray!40},\n",
    "#            outer/.style={outer sep=0},\n",
    "#            label/.style={align=center,font=\\bfseries\\small\\sffamily,text height=0.5em}}\n",
    "#\n",
    "#      % input labels\n",
    "#      \\foreach \\i [count=\\step from 1] in {Musika,maite,dut,{$<$/S$>$}} {\n",
    "#        \\node[emptystate]    (EncI\\step)  at (1.5*\\step-1.5, 0)  {\\i};\n",
    "#      }\n",
    "#      \\foreach \\i [count=\\step from 1] in {{$<$S$>$},I,love,music} {\n",
    "#        \\node[emptystate]    (DecI\\step)  at (1.5*\\step+6.0, 0)  {\\i};\n",
    "#      }\n",
    "#\n",
    "#      % embedding layers\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,encembed] (EncE\\step)  at (1.5*\\step-1.5, 1.5) {};\n",
    "#        \\node[state,decembed] (DecE\\step)  at (1.5*\\step+6.0, 1.5) {};\n",
    "#        \\draw[->]             (EncI\\step)  to (EncE\\step);\n",
    "#        \\draw[->]             (DecI\\step)  to (DecE\\step);\n",
    "#      }\n",
    "#\n",
    "#      % encoder LSTMs\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,encoder] (EncLA\\step)  at (1.5*\\step-1.5, 2.5) {};\n",
    "#        \\draw[->]            (EncE\\step)  to (EncLA\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.east] (EncLA_be\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.west] (EncLA_bw\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.east] (EncLA_ae\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.west] (EncLA_aw\\step);\n",
    "#      }\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\draw[densely dashed, ->]   (EncLA_be\\step) to (EncLA_bw\\next);\n",
    "#        \\draw[densely dashed, ->]   (EncLA_aw\\next) to (EncLA_ae\\step);\n",
    "#      }\n",
    "#\n",
    "#      % encoded vectors\n",
    "#      \\node[addstate]        (EncVecA)  at (6.0, 2.5)   {$\\oplus$};\n",
    "#      \\draw[densely dashed, ->]     (EncLA_be4) to (EncVecA);\n",
    "#      \\draw[densely dashed, ->, rounded corners=5pt]     (EncLA_aw1) -|([shift={(-5mm,3mm)}]EncLA1.north west) -- ([shift={(-10mm,4.2mm)}]EncVecA.north west) to (EncVecA.north west);\n",
    "#\n",
    "#\n",
    "#      % decoder LSTMs\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,decoder] (DecLA\\step)  at (1.5*\\step+6.0, 2.5) {};\n",
    "#        \\draw[->]            (DecE\\step)  to (DecLA\\step);\n",
    "#      }\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\draw[densely dashed, ->]   (DecLA\\step.east) to (DecLA\\next.west);\n",
    "#      }\n",
    "#      \\draw[densely dashed, ->]     (EncVecA.east)    to (DecLA1.west);\n",
    "#\n",
    "#      % dense layer\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\node[state,decoder] (DecD\\step)  at (1.5*\\step+6.0, 3.5) {};\n",
    "#        \\draw[->]            (DecLA\\step) to (DecD\\step);\n",
    "#      }\n",
    "#\n",
    "#      % output labels\n",
    "#      \\foreach \\i [count=\\step from 1] in {I,love,music,{$<$/S$>$}} {\n",
    "#        \\node[emptystate]    (DecO\\step)  at (1.5*\\step+6.0, 5.0)  {\\i};\n",
    "#        \\draw[->]            (DecD\\step)  to (DecO\\step);\n",
    "#      }\n",
    "#\n",
    "#      % input-feeding connections\n",
    "#      \\foreach \\step in {1,...,3} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\node[emptystate]   (Midway\\step) at (1.5*\\step+6.75, 3.75) {};\n",
    "#        \\draw[densely dotted, ->, rounded corners=2pt]    (DecO\\step.east) -|(1.5*\\step+6.75, 3.75) |-(DecI\\next.west);\n",
    "#      }\n",
    "#\n",
    "#      % figure labels\n",
    "#      \\node[label,text=red!80!black!60]          (asd)   at (5.0, 4.5)  {This is a\\\\bottleneck!};\n",
    "#      \\draw[->,color=red!80!black!60] (asd) to (EncVecA);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 1 -f svg --save mt_figures/encdec.svg\n",
    "#\n",
    "#   \\tikzset{state/.style={draw,rectangle,minimum height=2em,minimum width=3.5em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em,text height=1em,text depth=0.15em},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.2em},\n",
    "#            encoder/.style={minimum width=6cm,minimum height=1.5cm,fill=green!40!gray!40,font=\\bfseries\\sffamily},\n",
    "#            decoder/.style={minimum width=6cm,minimum height=1.5cm,fill=blue!40!gray!40,font=\\bfseries\\sffamily},\n",
    "#            outer/.style={outer sep=0},\n",
    "#            label/.style={align=center,font=\\itshape\\small}}\n",
    "#\n",
    "#      \\node[emptystate]      (I1)  at (2.5,  0)         {{Musika maite dut}};\n",
    "#\n",
    "#      \\node[state,encoder]           (ENC)  at (2.5,  2)         {{Encoder}};\n",
    "#      \\draw [->]  (I1) to (ENC.south);\n",
    "#\n",
    "#      \\node[emptystate]      (O1) at (11,  4)         {I love music};\n",
    "#\n",
    "#      \\node[state,decoder]           (DEC)  at (11, 2)         {{Decoder}};\n",
    "#      \\draw [->]  (DEC.north) to (O1);\n",
    "#\n",
    "#      \\draw [->]  (ENC) to (DEC);\n",
    "#\n",
    "#      \\node[emptystate]    (V) at (4.0, 4.0)     {\\sffamily\\textit{vector representation}};\n",
    "#\n",
    "#      \\node[emptystate]    (ED) at (6.75,   1.75)     {};\n",
    "#      \\draw [densely dashed]  (V.east) to[out=0,in=90] (ED.north);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 1 -f svg --save mt_figures/encdec_att.svg\n",
    "#\n",
    "# \\tikzset{state/.style={draw,rectangle,minimum height=1.5em,minimum width=2em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em},\n",
    "#            addstate/.style={draw,circle,inner sep=0.1em,fill=gray!10},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.2em},\n",
    "#            encembed/.style={fill=green!40!gray!40},\n",
    "#            decembed/.style={fill=blue!40!gray!40},\n",
    "#            encoder/.style={fill=green!40!gray!40},\n",
    "#            decoder/.style={fill=blue!40!gray!40},\n",
    "#            outer/.style={outer sep=0},\n",
    "#            attention/.style={minimum width=3cm,minimum height=1cm,text height=0.6em,text depth=0.2em,align=center,font=\\bfseries\\sffamily,fill=red!60!gray!60}}\n",
    "#\n",
    "#      % input labels\n",
    "#      \\foreach \\i [count=\\step from 1] in {{$<$S$>$},Musika,maite,dut,{$<$/S$>$}} {\n",
    "#        \\node[emptystate]    (EncI\\step)  at (1.5*\\step-1.5, 1.0)  {\\i};\n",
    "#      }\n",
    "#\n",
    "#      % embedding layer\n",
    "#      \\foreach \\step in {1,...,5} {\n",
    "#        \\node[state,encembed] (EncE\\step)  at (1.5*\\step-1.5, 2.0) {};\n",
    "#        \\draw[->]             (EncI\\step)  to (EncE\\step);\n",
    "#      }\n",
    "#\n",
    "#      % encoder LSTMs\n",
    "#      \\foreach \\step in {1,...,5} {\n",
    "#        \\node[state,encoder] (EncLA\\step)  at (1.5*\\step-1.5, 3.0) {};\n",
    "#        \\draw[->]            (EncE\\step)  to (EncLA\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.east] (EncLA_be\\step);\n",
    "#        \\coordinate[below=0.1 of EncLA\\step.west] (EncLA_bw\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.east] (EncLA_ae\\step);\n",
    "#        \\coordinate[above=0.1 of EncLA\\step.west] (EncLA_aw\\step);\n",
    "#      }\n",
    "#      \\foreach \\step in {1,...,4} {\n",
    "#        \\pgfmathtruncatemacro{\\next}{add(\\step,1)}\n",
    "#        \\draw[densely dashed, ->]   (EncLA_be\\step) to (EncLA_bw\\next);\n",
    "#        \\draw[densely dashed, ->]   (EncLA_aw\\next) to (EncLA_ae\\step);\n",
    "#      }\n",
    "#\n",
    "#      % attentional model\n",
    "#      \\node[state,attention]   (Att)  at (1.5,  5.5)  {Attention model};\n",
    "#      \\node[addstate]          (Mult) at (5.25, 5.5)  {$\\times$};\n",
    "#      \\foreach \\step in {1,...,5} {\n",
    "#        \\draw[->]      (EncLA\\step.north) to (Att);\n",
    "#        \\draw[->]      (EncLA\\step.north) to (Mult);\n",
    "#      }\n",
    "#      \\draw[densely dashed, ->] (Att) to (Mult);\n",
    "#\n",
    "#\n",
    "#      % decoder\n",
    "#      \\node[emptystate]     (DecO1)      at (0.0, 11.5)    {love};\n",
    "#      \\node[emptystate]     (DecO2)      at (3.6, 11.5)    {music};\n",
    "#      \\foreach \\i [count=\\step from 1] in {I,love} {\n",
    "#        \\node[emptystate]     (DecI\\step)  at (3.6*\\step-3.6,  7.5)   {\\i};\n",
    "#        \\node[state,decembed] (DecE\\step)  at (3.6*\\step-3.6,  8.5)   {};\n",
    "#        \\node[state,decoder]  (DecL\\step)  at (3.6*\\step-3.6,  9.5)   {};\n",
    "#        \\node[state,decoder]  (DecD\\step)  at (3.6*\\step-3.6, 10.5)   {};\n",
    "#      }\n",
    "#\n",
    "#      \\node[emptystate]        (DecI0)      at (-1.5, 7.5) {$<$S$>$};\n",
    "#      \\node[state,decembed]    (DecE0)      at (-1.5, 8.5) {};\n",
    "#      \\node[state,decoder]     (DecL0)      at (-1.5,  9.5) {};\n",
    "#      \\node[state,decoder]     (DecD0)      at (-1.5, 10.5) {};\n",
    "#      \\node[emptystate]        (DecO0)      at (-1.5, 11.5) {I};\n",
    "#\n",
    "#      \\foreach \\step in {0,...,2} {\n",
    "#        \\draw[->]             (DecI\\step)  to (DecE\\step);\n",
    "#        \\draw[->]             (DecE\\step)  to (DecL\\step);\n",
    "#        \\draw[->]             (DecL\\step)  to (DecD\\step);\n",
    "#        \\draw[->]             (DecD\\step)  to (DecO\\step);\n",
    "#      }\n",
    "#\n",
    "#      \\node[emptystate]     (DecLx)      at (5.1,  9.5) {};\n",
    "#      \\draw[densely dashed, ->]  (DecL0) to (DecL1);\n",
    "#      \\draw[densely dashed, ->]  (DecL1) to (DecL2);\n",
    "#      \\draw[densely dashed, ->]  (DecL2) to (DecLx);\n",
    "#      \\draw[densely dashed, ->, rounded corners=12pt]  (DecL1) -| (Att);\n",
    "#      \\draw[densely dashed, ->, rounded corners=12pt]  (Mult.north) |-([shift={(5mm,8mm)}]Att.north) |- (DecL2.west);\n",
    "#\n",
    "#      \\node[emptystate]     at (0.8, 7)  {};\n",
    "#      \\node[emptystate]     at (6.55, 6.85)  {\\sffamily context vector $z_t$};\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %%tikz -l arrows,positioning -s 1100,500 -sc 1 -f svg --save mt_figures/align.svg\n",
    "#\n",
    "#   \\tikzset{state/.style={draw,rectangle,minimum height=2em,minimum width=3.5em,\n",
    "#                          inner xsep=1em,inner ysep=0.5em,text height=1em,text depth=0.15em},\n",
    "#            emptystate/.style={inner sep=0.4em,text height=0.6em,text depth=0.1em},\n",
    "#            label/.style={align=center,font=\\itshape\\small}}\n",
    "#\n",
    "#      \\node[emptystate]      (I1)  at (0,  0)         {{Musika}};\n",
    "#      \\node[emptystate]      (I2)  at (1.2,  0)         {{maite}};\n",
    "#      \\node[emptystate]      (I3)  at (2.1,  0)         {{dut}};\n",
    "#\n",
    "#      \\node[emptystate]      (O1)  at (0,  2)         {{I}};\n",
    "#      \\node[emptystate]      (O2)  at (0.6,  2)         {{love}};\n",
    "#      \\node[emptystate]      (O3)  at (1.6,  2)         {{music}};\n",
    "#\n",
    "#\n",
    "#      \\draw [-]  (I1.north) to (O3.south);\n",
    "#      \\draw [-]  (I3.north) to (O1.285);\n",
    "#      \\draw [-]  (I2.north) to (O2.south);\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}