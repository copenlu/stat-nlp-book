{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T17:30:29.181539",
     "start_time": "2016-12-02T17:30:29.172204"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'controls': 'false',\n",
       " 'progress': 'true',\n",
       " 'theme': 'white',\n",
       " 'transition': 'none'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reveal.js\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'theme': 'white',\n",
    "        'transition': 'none',\n",
    "        'controls': 'false',\n",
    "        'progress': 'true',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representing Words as Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-02T16:54:38.546624",
     "start_time": "2016-12-02T16:54:38.540051"
    },
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "* Walkthrough of lecture preparation quiz\n",
    "* Representations of Words\n",
    "    * Motivation\n",
    "    * Sparse Binary Representations\n",
    "    * Dense Continuous Representations\n",
    "* Unsupervised Learning of Word Representations\n",
    "    * Motivation\n",
    "    * Sparse Co-occurence Representations\n",
    "    * Neural Word Representations\n",
    "* Additional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TODO: Update before lecture\n",
    "<center><img src=\"../img/quiz_errors.png\"></center>\n",
    "<center><img src=\"../img/nand.jpg\" width=\"40%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feed-forward Neural Networks\n",
    "<center><img src=\"../img/mlp.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"../img/backprop.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word representations ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Word representations visualised in two dimensions](../img/word_representations.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why talk about representations? ##\n",
    "\n",
    "* Machine Learning, features are representations\n",
    "* Better representations, better performance\n",
    "* Representation Learning (\"Deep Learning\"), trendy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What makes a good representation? ##\n",
    "\n",
    "1. Representations are **distinct**\n",
    "2. **Similar** words have **similar** representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Formal Task ##\n",
    "\n",
    "* Words: $w$\n",
    "* Vocabulary: $\\mathbb{V} (\\forall_{i} w_{i} \\in \\mathbb{V})$\n",
    "* Find representation function: $f(w_{i}) = r_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Binary Representations ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Binary Representations ##\n",
    "\n",
    "* Map words to unique positive non-zero integers\n",
    "* $f_{id}(w) \\mapsto \\mathbb{N^{*}}$\n",
    "* $g(w, i) = {\\left\\lbrace\n",
    "    \\begin{array}{ll}\n",
    "        1 & \\textrm{if }~i = f_{id}(w) \\\\\n",
    "        0 & \\textrm{otherwise} \\\\\n",
    "    \\end{array}\\right.}$\n",
    "* \"One-hot\" vector\n",
    "* $f_{sb}(w) = (g(w, 1), \\ldots, g(w, |V|))$\n",
    "* $f_{sb}(w) \\mapsto \\{0,1\\}^{|V|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Binary Example ##\n",
    "\n",
    "* $\\mathbb{V} = \\{\\textrm{apple}, \\textrm{orange}, \\textrm{rabbit}\\}$\n",
    "* $f_{id}(\\textrm{apple}) = 1, \\ldots$, $f_{id}(\\textrm{rabbit}) = 3$\n",
    "* $f_{sb}(\\textrm{apple}) = (1, 0, 0)$\n",
    "* $f_{sb}(\\textrm{orange}) = (0, 1, 0)$\n",
    "* $f_{sb}(\\textrm{rabbit}) = (0, 0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Binary Visualised ##\n",
    "\n",
    "![Sparse binary representations visualised](../img/sparse_binary.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cosine Similarity ##\n",
    "\n",
    "* $cos(u, v) = \\frac{u \\cdot v}{||u|| ||v||}$\n",
    "* $cos(u, v) \\mapsto [-1, 1]$\n",
    "* $cos(u, v) = 1$; identical\n",
    "* $cos(u, v) = -1$; opposites\n",
    "* $cos(u, v) = 0$; orthogonal\n",
    "\n",
    "Note the different formulation in SciPy: $cos(u, v) = 1 - \\frac{u \\cdot v}{||u|| ||v||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cosine Similarity Visualised ##\n",
    "\n",
    "<center><img src=\"http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png\" width=\"110%\"></center>\n",
    "\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Binary Similarities ##\n",
    "\n",
    "* $cos(f_{sb}(\\textrm{apple}), f_{sb}(\\textrm{rabbit})) = 0$\n",
    "* $cos(f_{sb}(\\textrm{apple}), f_{sb}(\\textrm{orange})) = 0$\n",
    "* $cos(f_{sb}(\\textrm{orange}), f_{sb}(\\textrm{rabbit})) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Continuous Representations ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Continuous Representations ##\n",
    "\n",
    "* $f_{id}(w) \\mapsto \\mathbb{N}^{*}$\n",
    "* \"Embed\" words as matrix rows\n",
    "* Dimensionality: $d$ (hyperparameter)\n",
    "* $W \\in \\mathbb{R}^{|\\mathbb{V}| \\times d}$\n",
    "* $f_{dc}(w) = W_{f_{id}(w), :}$\n",
    "* $f_{dc}(w) \\mapsto \\mathbb{R}^{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Continuous Example ##\n",
    "\n",
    "* $\\mathbb{V} = \\{\\textrm{apple}, \\textrm{orange}, \\textrm{rabbit}\\}$\n",
    "* $d = 2$\n",
    "* $W \\in \\mathbb{R}^{3 \\times 2}$\n",
    "* $f_{id}(\\textrm{apple}) = 1, \\ldots, f_{id}(\\textrm{rabbit}) = 3$\n",
    "* $f_{dc}(\\textrm{apple}) = (1.0, 1.0)$\n",
    "* $f_{dc}(\\textrm{orange}) = (0.9, 1.0)$\n",
    "* $f_{dc}(\\textrm{rabbit}) = (0.1, 0.5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Continuous Visualised ##\n",
    "\n",
    "<center><img src=\"../img/dense_continuous.svg\" width=\"80%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Continuous Similarities ##\n",
    "\n",
    "* $cos(f_{dc}(\\textrm{apple}),f_{dc}(\\textrm{rabbit})) \\approx 0.83$\n",
    "* $cos(f_{dc}(\\textrm{apple}),f_{dc}(\\textrm{orange})) \\approx 1.0$\n",
    "* $cos(f_{dc}(\\textrm{orange}),f_{dc}(\\textrm{rabbit})) \\approx 0.86$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning of Word Representations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not supervised? ##\n",
    "\n",
    "<center><img src=\"../img/annotated_vs_unannotated_data.svg\" width=\"40%\"></center>\n",
    "\n",
    "\n",
    "* Also, inherent incompleteness..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linguistic Inspirations ##\n",
    "\n",
    "* \"Oculist and eye-doctor … occur in almost the same environments. … If $A$ and $B$ have almost identical environments we say that they are synonyms.\" – Zellig Harris (1954)\n",
    "* \"You shall know a word by the company it keeps.\" – John Rupert Firth (1957)\n",
    "* Akin to \"meaning is use\" – Wittgenstein (1953)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Co-occurence Representations ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Co-occurences ##\n",
    "\n",
    "* Collected from a large collection of *raw* text\n",
    "* E.g. Wikipedia, crawled news data, tweets, ...\n",
    "\n",
    "1. \"…comparing an **apple** to an **orange**…\"\n",
    "2. \"…an **apple** and **orange** from Florida…\"\n",
    "3. \"…my **rabbit** is not shaped like an **orange**…\" (yes, there is always **noise** in the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Co-occurence Representations ##\n",
    "\n",
    "* The number of times words co-occur in a text collection\n",
    "* $C \\in \\mathbb{N}^{|V| \\times |V|}$\n",
    "* $f_{id}(\\textrm{apple}) = 1, \\ldots, f_{id}(\\textrm{rabbit}) = 3$\n",
    "* $C = \\begin{pmatrix}\n",
    "        2 & 2 & 0 \\\\\n",
    "        2 & 3 & 1 \\\\\n",
    "        0 & 1 & 1 \\\\\n",
    "    \\end{pmatrix}$\n",
    "* $f_{cs}(w) = C_{f_{id}(w), :}$\n",
    "* $f_{cs}(w) \\mapsto \\mathbb{N}^{|V|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Co-occurence Example ##\n",
    "\n",
    "* $\\mathbb{V} = \\{\\textrm{apple}, \\textrm{orange}, \\textrm{rabbit}\\}$\n",
    "* $f_{id}(\\textrm{apple}) = 1, \\ldots, f_{id}(\\textrm{rabbit}) = 3$\n",
    "* $f_{cs}(\\textrm{apple}) = (2, 2, 0)$\n",
    "* $f_{cs}(\\textrm{orange}) = (2, 3, 1)$\n",
    "* $f_{cs}(\\textrm{rabbit}) = (0, 1, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Co-occurence Similarities ##\n",
    "\n",
    "* $cos(f_{cs}(\\textrm{apple}), f_{cs}(\\textrm{rabbit})) \\approx 0.50$\n",
    "* $cos(f_{cs}(\\textrm{apple}), f_{cs}(\\textrm{orange})) \\approx 0.94$\n",
    "* $cos(f_{cs}(\\textrm{orange}), f_{cs}(\\textrm{rabbit})) \\approx 0.76$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Word Representations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning by Slot Filling ##\n",
    "\n",
    "* \"…I had some **_____** for breakfast today…\"\n",
    "* Good: *cereals*\n",
    "* Bad: *airplanes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img width=800 src=\"../img/cbow_sg.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Loss Function ##\n",
    "\n",
    "* $w \\in \\mathbb{V}$; $c \\in \\mathbb{V}$\n",
    "* $D = ((c, w),\\ldots)$; observed co-occurences\n",
    "* $D' = ((c, w),\\ldots)$; \"noise samples\"\n",
    "* $\\textrm{max}~p((c, w) \\in D | W) - p((c, w) \\in D' | W)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Skip-Gram Model ##\n",
    "\n",
    "* $W^{w} \\in \\mathbb{R}^{|\\mathbb{V}| \\times d}$; $W^{c} \\in \\mathbb{R}^{|\\mathbb{V}| \\times d}$\n",
    "* $D = ((c, w),\\ldots)$; $D' = ((c, w),\\ldots)$\n",
    "* $\\sigma(x) = \\frac{1}{1 + \\textrm{exp}(-x)}$\n",
    "* $p((c, w) \\in D | W^{w}, W^{c}) = \\sigma(W^{c}_{f_{id}(c),:} \\cdot W^{w}_{f_{id}(w),:})$\n",
    "* $\\arg\\max\\limits_{W^{w},W^{c}} \\sum\\limits_{(w,c) \\in D} \\log \\sigma(W^{c}_{f_{id}(c),:} \\cdot W^{w}_{f_{id}(w),:}) \\\\ + \\sum\\limits_{(w,c) \\in D'} \\log \\sigma(-W^{c}_{f_{id}(c),:} \\cdot W^{w}_{f_{id}(w),:})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Representation ##\n",
    "\n",
    "* Learned using [word2vec](https://code.google.com/p/word2vec/)\n",
    "* Google News data, $~1,000,000,000$ words\n",
    "* $|\\mathbb{V}| = 3,000,000$\n",
    "* $d = 300$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Representation Example ##\n",
    "\n",
    "* $f_{n}(\\textrm{apple}) = (-0.06, -0.16, \\ldots, 0.34)$\n",
    "* $f_{n}(\\textrm{orange}) = (-0.10, -0.18, \\ldots, 0.08)$\n",
    "* $f_{n}(\\textrm{rabbit}) = (0.02, 0.11, \\ldots, 0.11)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Representation Similarities ##\n",
    "\n",
    "* $cos(f_{n}(\\textrm{apple}), f_{n}(\\textrm{rabbit})) \\approx 0.34$\n",
    "* $cos(f_{n}(\\textrm{apple}), f_{n}(\\textrm{orange})) \\approx 0.39$\n",
    "* $cos(f_{n}(\\textrm{orange}), f_{n}(\\textrm{rabbit})) \\approx 0.20$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Representations Visualised ##\n",
    "\n",
    "<center><img src=\"../img/word_representations.svg\" width=\"60%\"></center>\n",
    "\n",
    "* Dimensionality reduction using [t-SNE](https://lvdmaaten.github.io/tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Representations Visualised (zoomed) ##\n",
    "\n",
    "![Word representations visualised in two dimensions, zoomed in on a small cluster](../img/word_representations_zoom.svg)\n",
    "\n",
    "* Dimensionality reduction using [t-SNE](https://lvdmaaten.github.io/tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Representation Algebra ##\n",
    "\n",
    "* $f_{n}(\\textrm{king}) - f_{n}(\\textrm{man}) + f_{n}(\\textrm{woman}) \\approx f_{n}(\\textrm{queen})$\n",
    "* $f_{n}(\\textrm{Paris}) - f_{n}(\\textrm{France}) + f_{n}(\\textrm{Italy}) \\approx f_{n}(\\textrm{Rome})$\n",
    "\n",
    "<center><img src=\"../img/regularities.png\" width=\"130%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"../img/quiz_time.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contextualised Representations #\n",
    "\n",
    "* Standard embedding representations have *one* representation per word, regardless of the *current* context\n",
    "\n",
    "* Contextualised Representations use the context surrounding the word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contextualised Representations Example ##\n",
    "\n",
    "\n",
    "* \"Yesterday I saw a bass ...\"\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"../img/bass_1.jpg\" width=\"300\" />\n",
    "  <img src=\"../img/bass_2.svg\" width=\"100\" /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contextualised Representations Example ##\n",
    "\n",
    "\n",
    "* a) \"Yesterday I saw a bass swimming in the lake\"\n",
    "* b) \"Yesterday I saw a bass in the music shop\"\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"../img/bass_1.jpg\" width=\"300\" />\n",
    "  <img src=\"../img/bass_2.svg\" width=\"100\" /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contextualised Representations Example ##\n",
    "\n",
    "\n",
    "* a) <span style=\"color:red\">\"Yesterday I saw a bass swimming in the lake\"</span>.\n",
    "* b) <span style=\"color:green\">\"Yesterday I saw a bass in the music shop\"</span>.\n",
    "\n",
    "<p float=\"center\">\n",
    "  <img src=\"../img/bass_visualisation.jpg\" width=\"500\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What makes a good representation these days? ##\n",
    "\n",
    "1. Representations are **distinct**\n",
    "2. **Similar** words have **similar** representations\n",
    "3. Representations take **context** into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using pre-trained representations from BERT #\n",
    "\n",
    "\n",
    "Some basic code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/johannesbjerva/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/johannesbjerva/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/johannesbjerva/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to desactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproductible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "tokens_tensor = tokens_tensor\n",
    "segments_tensors = segments_tensors\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # PyTorch-Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"../img/quiz_time.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary #\n",
    "\n",
    "* Moving from features to \"representations\"\n",
    "* Representations limits what we can learn and our generalisation power\n",
    "* Many ways to learn representations (there are more than what we covered)\n",
    "* Neural representations most widely used\n",
    "    * 'static' representations are widespread\n",
    "    * Contextualised representations usually give much more expressivity\n",
    "* Adding representations trained unsupervisedly improves performance on supervised tasks (semi-supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Reading #\n",
    "\n",
    "* [\"Word Representations: A Simple and General Method for Semi-Supervised Learning\"](http://www.aclweb.org/anthology/P/P10/P10-1040.pdf) by Turian et al. (2010)\n",
    "* [\"Representation Learning: A Review and New Perspectives\"](https://arxiv.org/abs/1206.5538) by Bengio et al. (2012)\n",
    "* [\"Linguistic Regularities in Continuous Space Word Representations\"](http://www.aclweb.org/anthology/N/N13/N13-1090.pdf) by Mikolov et al. (2013a) ([video](http://techtalks.tv/talks/linguistic-regularities-in-continuous-space-word-representations/58471/))\n",
    "* [\"Distributed Representations of Words and Phrases and their Compositionality\"](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) by Mikolov et al. (2013b)\n",
    "* [\"word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method\"](https://arxiv.org/abs/1402.3722) by Goldberg and Levy (2014)\n",
    "* [\"Neural Word Embedding as Implicit Matrix Factorization\"](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) by Levy and Goldberg (2014)\n",
    "* [Blog post explaining BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) by Horev (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
